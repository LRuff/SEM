---
output: 
  pdf_document:
    fig_caption: true
    keep_tex: true
    latex_engine: pdflatex
    number_sections: true
header-includes:
- \usepackage[english]{babel}
- \usepackage{setspace}
- \usepackage{bm}
- \hyphenation{}
- \pagenumbering{arabic}
- \numberwithin{equation}{section}
- \newcommand{\Yobs}{Y_{\text{obs}}}
- \newcommand{\yobs}{y_{\text{obs}}}
- \newcommand{\Ymis}{Y_{\text{mis}}}
- \newcommand{\ymis}{y_{\text{mis}}}
- \newcommand{\E}{\mathbb{E}}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\argmax}{\text{argmax}}
- \newcommand{\I}{\mathcal{I}}
fontsize: 10pt
geometry: a4paper, left=30mm, right=30mm, top=20mm, bottom=20mm, includefoot
bibliography: references.bib
documentclass: article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<!-- Titlepage -->
\thispagestyle{empty}

\begin{raggedright}
  Freie Universit√§t Berlin\\
  Chair of Statistics\\
  Location: Berlin\\
  Winter Term 2016/2017\\
  Lecture: Missing Data Analysis\\
  Examiner: Dr. Florian Meinfelder
\end{raggedright}

\vspace*{\fill}
  \begin{center}
    \textbf{\Large{The Supplemented EM algorithm}}%
  \end{center}
\vfill

\begin{raggedleft}
  Carlo Michaelis (5043128), \href{mailto:carlo.michaelis@gmail.com}{carlo.michaelis@gmail.com}\\
  Lukas Ruff (5029054), \href{mailto:contact@lukasruff.com}{contact@lukasruff.com}\\
  \ \\
  Master Statistics\\
  \today\\
\end{raggedleft}

\newpage


<!-- Table of Contents -->
\tableofcontents
\newpage


<!-- List of Figures and Tables -->
\listoffigures
\listoftables
\newpage


<!-- Document Body -->
# Introduction

There are several methods to deal with missing data. Beside multiple imputation it is possible to estimate parameters by using the likelihood function. Unfortunatly in many cases, solving the likelihood of the observed data is not an easytask. Therefore the so called expectation maximization (EM) algorithm can be used. It was developed by @dempster1977maximum to obtain maximum likelihood estimations when the underlying distribution depends on unobserved variables.

While the algortihm itself - finding parameters $\hat\theta$ - is feasible, the estimtaion of the variance of the estimators (e.g. to obtain standard errors) is somehow difficult. Different approaches were suggested to face this problem:

 * Algebraic analysis [@meilijson1989fast]
 * Monte Carlo evaluation using multiple imputation [@rubin2004multiple]
 * Numerical methods [@carlin1987seasonal]
 * Resampling: Bootstrap/Jackknife
 * Supplemented EM (SEM)

While methods like resampling or algebraic analysis mostly depends on iid data, SEM can be done without this assumption. Furthermore numerical methods try to calculate the variance covariance matrix directly, while SEM just computes the increase of variance, which is computational more stable. Lastly Monte Carlo evaluation need more extra code and the result depends on the number of imputations. In 1990s, when the SEM was developped by @meng1991sem, the computational power was far less than today, where it was not possible to do as many imputation as are possible today.

In this term paper the goal is to describe the EM and SEM algorithm. The theory is done in general and afterwards applied to a bivariate normal problem. Furthermore we computed an example in `R` regarding the bivariate problem, which should give an intuition for the EM and the SEM algorithm.


# A Recap of the EM Algorithm

The Expectation Maximization (EM) algorithm is an iterative procedure to maximize the likelihood of a model with missing or latent variables. It consists of two steps, namely the *expectation step* and the *maximization step*. Both steps are applied alternately until some convergence criterion is reached. Depending on the given data structure, especially the missing data, we can find a *rate of convergence* $DM$ for the parameters. Both aspects of the EM algorithm are explained in this section.

## Objective and algorithm

Let the model of the complete data $\bm Y$, which has an observed part $\bm \Yobs$ and missing values $\bm \Ymis$, that is $\bm Y = (\bm \Yobs, \bm \Ymis)$, be described by the complete-data density $f(\bm Y \, | \, \bm \theta)$, $\bm \theta = (\theta_1, ..., \theta_d)$, and denote the complete-data log-likelihood by

$$
L(\bm \theta \, | \, \bm Y) = \log f(\bm Y \, | \, \bm \theta),
$$

that is we assume the missing-data mechanism to be ignorable. The EM algorithm finds the maximum likelihood estimate (MLE) of the parameter $\bm \theta$ of the *observed* likelihood $f(\bm \Yobs \, | \, \bm \theta)$, which we define by $\bm{\hat \theta}$. The key feature of the EM algorithm is, that this maximization of the observed (log-)likelihood is not carried out directly, but indirectly and iteratively by maximizing the conditional expectation over $f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta^{(t)})$ of the complete-data log-likelihood $L(\bm \theta \, | \, \bm Y)$, where $\bm \theta^{(t)}$ is the parameter value at the $t$-th iteration. The algorithm starts with some initial parameter value $\bm \theta^{(0)}$.

**Expectation step**. The expectation step (*E step*) computes the conditional expectation of the complete-data log-likelihood over $f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta^{(t)})$ using the parameter of iteration $t$, that is

$$
Q(\bm \theta \, | \, \bm \theta^{(t)}) = \E[L(\bm \theta \, | \, \bm Y) \, | \, \bm \Yobs, \bm \theta^{(t)}]  = \int L(\bm \theta \, | \, \bm Y) f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta^{(t)}) \, d\bm \Ymis.
$$

Therefore, the E step estimates the missing data parts of the complete-data log-likelihood by the expectational value using the observed data and the current best parameter estimates $\bm \theta^{(t)}$ of the model.


**Maximization step**. In the maximization step (*M step*) the objective of the E step is maximized in $\bm \theta$ in order to obtain an update of the model parameters, which means solving

$$
\bm \theta^{(t+1)} = \underset{\bm \theta}{\argmax} \,Q(\bm \theta \, | \, \bm \theta^{(t)}), \qquad \text{ i.e. } \, Q(\bm \theta^{(t+1)} \, | \, \bm \theta^{(t)}) \geq Q(\bm \theta \, | \, \bm \theta^{(t)}), \quad \forall \bm \theta.
$$

The updated parameters $\bm \theta^{(t+1)}$ are always better in the sense, that they have a greater or equal observed-data (log-)likelihood than the previous parameters $\bm \theta^{(t+1)}$, that is $L(\bm \theta^{(t+1)} \, | \, \bm \Yobs) \geq L(\bm \theta^{(t)} \, | \, \bm \Yobs)$.[^1]
After obtaining the parameter update $\bm \theta^{(t+1)}$, it can be used in the following E and M step which conduct the next EM iteration.  

For the large class of exponential families, the EM algorithm is usually easy to implement: The M step is identical to the computation of ML estimates from the complete-data log-likelihood $L(\bm \theta \, | \, \bm Y)$; the E step for most problems only involves calculating the conditional expectation of the sufficient statistics, which are easy to compute from the moments of the conditional distribution.

## Rate of convergence

The EM algorithm updates the parameter in every iteration of E step and M step from $\bm \theta^{(t)}$ to $\bm \theta^{(t+1)}$. To describe this behavior we can define a mapping

$$
M: \Theta \to \Theta,\; \bm \theta \mapsto M(\bm \theta)\quad \text{such that}\quad M(\bm \theta^{(t)}) = \bm \theta^{(t+1)}, \, t = 0, 1, \ldots \;
$$

where $\Theta$ is the parameter space. If $\left(\bm \theta^{(t)}\right)$ converges to $\bm{\hat \theta}$ and $M$ is continuous, we have

$$
M(\bm{\hat \theta}) = \bm{\hat \theta},
$$

that is $\bm{\hat \theta}$ is a fixed point of $M$. Thus, in the neighborhood of $\bm{\hat \theta}$, we get 

\begin{align*}
M(\bm \theta^{(t)}) &\approx M(\bm{\hat \theta}) + DM (\bm \theta^{(t)} - \bm{\hat \theta})\\
\iff \qquad \bm \theta^{(t+1)} &\approx \bm{\hat \theta} + DM (\bm \theta^{(t)} - \bm{\hat \theta})\\
\iff \, \bm \theta^{(t+1)} - \bm{\hat \theta} &\approx DM (\bm \theta^{(t)} - \bm{\hat \theta})
\end{align*}

doing a taylor approximation of $M(\bm \theta^{(t)})$ at $\bm{\hat \theta}$ stopping after the linear term, where

\begin{equation}
\label{eq:rate}
DM = \left.\left( \frac{\partial M_j(\bm \theta)}{\partial \theta_i} \right) \right|_{\bm \theta = \bm{\hat \theta}}
\end{equation}

is the $d \times d$ Jacobian matrix of $M(\bm \theta) = (M_1(\bm \theta), \ldots, M_d(\bm \theta))$ evaluated at $\bm{\hat \theta}$. Hence, in the neighborhood of $\bm{\hat \theta}$, the EM algorithm converges linearly with rate matrix $DM$.


# The SEM algorithm

In comparison to multiple imputation methods, asymptotic variance-covariance matrices of the parameter estimaton $\bm{\hat\theta}$ (e.g. standard errors) are not given automatically when using the EM algorithm. Instead, additional steps are required in order to compute standard errors of the estimates. One approach introduced by @meng1991sem is the so-called *supplemented EM* or *SEM* algorithm. This procedure obtains numerically stable asymptotic variance-covariance matrices only from computing the complete-data asymptotic variance-covariance matrix, using code for the E and M steps of EM, and standard matrix operations. The key idea of the SEM algorithm is effectively to numerically differentiate the implicit mapping $\bm \theta \mapsto M(\bm \theta)$ defined by the EM algorithm, i.e. defining a method to numerically approximate the Jacobian matrix $DM$.


## Asymptotic variance-covariance matrix of $(\bm \theta - \bm{\hat \theta})$ based on $\bm \Yobs$

The desired asymptotic variance-covariance matrix of $(\bm \theta - \bm{\hat \theta})$ based on $\bm \Yobs$ is given by the inverse of the observed information matrix, i.e.

$$
V = \I_{o}^{-1}(\bm{\hat \theta} \, | \, \bm \Yobs),
$$

where the observed information matrix $\I_{o}(\bm \theta \, | \, \bm \Yobs)$ is the negative Hessian of the log-likelihood of the observed data,

\begin{equation}
\label{eq:Iobs}
\I_{o}(\bm \theta \, | \, \bm \Yobs) = -\frac{\partial^2 \log f(\bm \Yobs \, | \, \bm \theta)}{\partial\bm \theta \cdot \partial \bm \theta}.
\end{equation}

The problem is that it can often be very difficult to evaluate (\ref{eq:Iobs}) directly. Therefore, another representation of $V$ is preferred. Precisely, we will derive the following decomposition of $V$:

\begin{equation}
\label{eq:V}
V = V_c + \Delta V \qquad \text{ with } \, V_c = \I_{oc}^{-1},
\end{equation}

where $V_c$ is the asymptotic complete-data variance-covariance matrix given by the inverse of the conditional expectation over $f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta)$ of the complete-data information matrix evaluated at $\bm \theta = \bm{\hat \theta}$, that is

\begin{equation}
\label{eq:Ioc}
\I_{oc} = \E[\I_{c}(\bm \theta \, | \, \bm Y) \, | \, \bm \Yobs, \bm \theta ] \bigg\rvert_{\bm \theta = \bm{\hat \theta}} \; , \qquad \I_{c}(\bm \theta \, | \, \bm Y) = -\frac{\partial^2 \log f(\bm Y \, | \, \bm \theta)}{\partial \bm \theta \cdot \partial \bm \theta},
\end{equation}

and $\Delta V$, the increase due to missing data, is given by

\begin{equation}
\label{eq:DeltaV}
\Delta V =  V_c \, DM \, (I_d - DM)^{-1},
\end{equation}

where $DM$ is the rate matix from (\ref{eq:rate}) and $I_d$ is the $d$-dimensional identity matrix. We will now show, why the decomposition in (\ref{eq:V}) holds.

<!-- TODO: How is the (well-known) asymptotic variance V derived? -->
<!-- explain why only large sample i.e. asymptotic -->
<!-- Explain relation to fisher information matrix (regularity / large sample) -->

## Proof that $V = V_c + \Delta V$

From Bayes' rule on densities, we have the factorization

$$
f(\bm Y \, | \, \bm \theta) = f(\bm \Yobs \, | \, \bm \theta) \, f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta),
$$

and therefore it follows for the log-likelihood of $\bm \theta$ given $\bm \Yobs$

\begin{align*}
L(\bm \theta \, | \, \bm \Yobs) &= \log f(\bm \Yobs \, | \, \bm \theta)\\
&= \log f(\bm Y \, | \, \bm \theta) - \log f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta) = L(\bm \theta \, | \, \bm Y) - \log f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta).
\end{align*}

Taking second derivatives and the expectation over the conditional distribution $f(\bm \Ymis \, | \, \bm \Yobs , \bm \theta)$, we further get that

\begin{align*}
&&\E\left[\frac{\partial^2 L(\bm \theta \, | \, \bm \Yobs)}{\partial \bm \theta \cdot \partial \bm \theta} \, | \, \bm \Yobs, \bm \theta \right] &= \E\left[\frac{\partial^2 L(\bm \theta \, | \, \bm Y)}{\partial \bm \theta \cdot \partial \bm \theta} \, | \, \bm \Yobs, \bm \theta \right] - \E\left[\frac{\partial^2 \log f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta)}{\partial \bm \theta \cdot \partial \bm \theta} \, | \, \bm \Yobs, \bm \theta \right]\\
\iff &&-\frac{\partial^2 L(\bm \theta \, | \, \bm \Yobs)}{\partial \bm \theta \cdot \partial \bm \theta} &= \E\left[-\frac{\partial^2 L(\bm \theta \, | \, \bm Y)}{\partial \bm \theta \cdot \partial \bm \theta} \, | \, \bm \Yobs, \bm \theta \right] - \E\left[-\frac{\partial^2 \log f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta)}{\partial \bm \theta \cdot \partial \bm \theta} \, | \, \bm \Yobs, \bm \theta \right] \\
\iff &&\I_o(\bm \theta \, | \, \bm \Yobs) &= \E\left[\I_c(\bm \theta \, | \, \bm Y) \, | \, \bm \Yobs, \bm \theta \right] - \E\left[-\frac{\partial^2 \log f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta)}{\partial\bm \theta \cdot \partial\bm \theta} \, | \, \bm \Yobs, \bm \theta \right].
\end{align*}

Evaluating this equation at $\bm \theta = \bm{\hat \theta}$ gives

\begin{equation}
\label{eq:informationprinciple}
\I_o(\bm{\hat \theta} \, | \, \bm \Yobs) = \I_{oc} - \I_{om},
\end{equation}

where 

$$
\I_{om} = \left. \E\left[-\frac{\partial^2 \log f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta)}{\partial\bm \theta \cdot \partial\bm \theta} \, | \, \bm \Yobs, \bm \theta \right] \right\rvert_{\bm \theta = \bm{\hat \theta}}
$$

can be viewed as the missing information. Equation (\ref{eq:informationprinciple}) is also called *missing information principle* with the nice interpretation

$$
\textit{observed information} = \textit{complete information } - \textit{missing information}.
$$

Besides, @dempster1977maximum showed that when maximizing $Q(\bm \theta \, | \, \bm \theta^{(t)})$ in the M step of the EM algorithm, i.e. when setting the first derivative of $Q(\bm \theta \, | \, \bm \theta^{(t)})$ w.r.t. $\bm \theta$ equal to zero, it holds that

$$
DM = \I_{om} \I_{oc}^{-1}.
$$

This can be used with (\ref{eq:informationprinciple}) to obtain

$$
\I_o(\bm{\hat \theta} \, | \, \bm \Yobs) = \I_{oc} - \I_{om} = (I_d - \I_{om}\I_{oc}^{-1})\I_{oc} = (I_d - DM)\I_{oc}.
$$

Finally, by inverting both sides of this equation, we get the decomposition of the asymptotic variance-covariance matrix based on $\bm \Yobs$:

\begin{align*}
V = \I_o^{-1}(\bm{\hat \theta} \, | \, \bm \Yobs) &= \I_{oc}^{-1}(I_d - DM)^{-1}\\
&= \I_{oc}^{-1}(I_d - DM + DM)(I_d - DM)^{-1}\\
&= \I_{oc}^{-1} + \I_{oc}^{-1} DM (I_d - DM)^{-1}\\
&= \I_{oc}^{-1} + \Delta V = V_c + \Delta V.
\end{align*}

This representation of $V$ is the ansatz of the SEM algorithm, which consists of three parts: (1) the evaluation of $V_c = \I_{oc}^{-1}$, (2) the numerical approximation of $DM$, and (3) using (\ref{eq:V}) with (\ref{eq:DeltaV}) to estimate $V$. We look into the evaluation of $V_c = \I_{oc}^{-1}$ in the next subsection.

<!-- Optional TODO: proof 2.4.5 -->


## Evaluation of $V_c$ for exponential families

We consider the evaluation of $V_c = \I_{oc}^{-1}$ for the case that the complete-data density $f(\bm Y \, | \, \bm \theta)$ is from an exponential family, since this is most common in practical applications. This means that the complete-data density has the following representation

$$
f(\bm Y \, | \, \bm \theta) = h(\bm Y) \exp\{\eta(\bm \theta)^T T(\bm Y) - A(\bm \theta)\} = h(\bm Y) \exp\left\{\sum_{i=1}^s\eta_i(\bm \theta) T_i(\bm Y) - A(\bm \theta) \right\},
$$

where the *natural parameter* $\eta(\bm \theta)$ and the *sufficient statistics* $T(\bm Y)$ are vector functions in $\R^s, s \geq d,$ and the *base measure* $h(\bm Y)$ as well as the *log-normalizer* $A(\bm \theta)$ are scalar functions.

The sufficient statistics $T(\bm Y)$ is crucial for the implementation and application of the EM and SEM algorithm for exponential families. This is because we have

$$
L(\bm \theta \, | \, \bm Y) = \log f(\bm Y \, | \, \bm \theta) = \log h(\bm Y) + \eta(\bm \theta)^T T(\bm Y) - A(\bm \theta)
$$

which means that the complete-data log-likelihood is linear in the sufficient statistics $T(\bm Y)$, i.e. $L(\bm \theta \, | \, \bm Y) = L(\bm \theta \, | \, T(\bm Y))$. This means that in order to compute the E step for exponential families, we practically just have to evaluate the conditional expectation of the sufficient statistics $\E[T(\bm Y) \, | \, \bm \Yobs, \bm \theta^{(t)}]$. Furthermore, linearity in the log-likelihood implies that the complete-data information is also linear in $T(\bm Y)$, that is $\I_{c}(\bm \theta \, | \, \bm Y) = \I_{c}(\bm \theta \, | \, T(\bm Y))$. Therefore, from the definition of $\I_{oc}$ in (\ref{eq:Ioc}), we get that

\begin{align*}
\I_{oc} &= \E[\I_{c}(\bm \theta \, | \, \bm Y) \, | \, \bm \Yobs, \bm \theta ] \bigg\rvert_{\bm \theta = \bm{\hat \theta}}\\
&= \I_{c}(\bm{\hat \theta} \, | \, T^*(\bm \Yobs)),
\end{align*}

by linearity in $T(\bm Y)$ with $T^*(\bm \Yobs) = \E[T(\bm Y) \, | \, \bm \Yobs, \bm{ \hat \theta}]$, which is obtained in the very last E step of the EM procedure. Hence, we can obtain $V_c = \I_{oc}^{-1}$ simply by plugging in the conditional expectation of the sufficient statistics $T^*(\bm \Yobs)$ computed in the very last E step into the inverse of $\I_{c}(\bm \theta \, | \, T(\bm Y))$ evaluated at $\bm \theta = \bm{\hat \theta}$.

When the exponential family is regular (i.e. when $s = d$ and the Jacobian of $\eta(\bm \theta)$ has full rank) the evaluation of $V_c = \I_{oc}^{-1}$ is even simpler. In this case, the complete-data information matrix $\I_{oc}$ is just the standard Fisher information matrix evaluated in $\bm{\hat \theta}$, i.e.

$$
\I_{oc} = \I(\bm{\hat \theta}), \qquad \I(\bm \theta) = \E[\I_c(\bm \theta \, | \, \bm Y) \, | \, \bm \theta].
$$

Inverting the Fisher information then gives $V_c$.

For the case when the complete-data density $f(\bm Y \, | \, \theta)$ is not from an exponential family, the complete-data log-likelihood $L(\bm \theta \, | \, \bm Y)$ is no longer linear in the sufficient statistics $T(\bm Y)$. This can make the evaluation of the E step difficult, because then $Q(\bm \theta \, | \, \bm \theta^{(t)})$ is generally not of closed form. One approach to tackle this problem is to linearize $L(\bm \theta \, | \, \bm Y)$ in the sufficient statistics using a Taylor expansion. See @dempster1977maximum section 3.2 for further details.


## Numerical Approximation of DM

Let $r_{ij}$ be the $(i, j)$-th element of the rate matrix $DM$, that is

$$
DM = \left.\left( \frac{\partial M_j(\bm \theta)}{\partial \theta_i} \right) \right|_{\bm \theta = \bm{\hat \theta}} = \left(r_{ij}\right), \quad i,j = 1,\ldots,d.
$$

Define $\bm \theta^{(t)}(i)$ to be the ML estimate $\bm{\hat \theta}$ with varied $i$-th component:

$$
\bm \theta^{(t)}(i) = (\hat \theta_1, \ldots, \hat \theta_{i-1}, \theta^{(t)}_i, \hat \theta_{i+1}, \ldots, \hat \theta_d).
$$

That is, only the $i$-th component is active and all others are fixed at the ML estimate. Then, from the definition of $r_{ij}$ and writing the componentwise derivatives as limits of the difference quotients, we have

\begin{align*}
r_{ij} &= \frac{\partial M_j (\bm{\hat \theta})}{\partial \theta_i}\\
&= \lim_{\theta_i \to \hat \theta_i} \frac{M_j(\hat \theta_1, \ldots, \hat \theta_{i-1}, \theta_i, \hat \theta_{i+1}, \ldots, \hat \theta_d) - M_j(\hat \theta)}{\theta_i - \hat \theta_i}\\
&= \lim_{t \to \infty} \underbrace{\frac{M_j(\bm \theta^{(t)}(i)) - \hat \theta_j}
{\theta_i^{(t)} - \hat \theta_i}}_{=: r_{ij}^{(t)}} = \lim_{t \to \infty} r_{ij}^{(t)},
\end{align*}

if the EM algorithm converges, i.e. when $\lim_{t \to \infty} \theta_i^{(t)} = \hat \theta_i$. Since $M(\bm \theta)$ is defined implicitly by the EM algorithm, the rate sequences $(r_{ij}^{(t)})_t$ can be computed using the code of the E and M steps of the EM algorithm. Hence, after obtaining the ML estimate $\bm{\hat \theta}$ from some estimation procedure (e.g. also the EM algorithm), we can numerically approximate $DM$ only from using code of the EM algorithm. We summarize the complete SEM algorithm in the next subsection.

<!-- Optional: Alternatives for approximation of DM -->


<!-- ## Notes -->
<!-- It is important to emphasize that the variance-covariance matrix obtained from SEM is based on the second derivatives of the observed-data log-likelihood and only is inferentially valid asymptotically, i.e. for large samples. -->
<!-- The more normal the likelihood, the better the approximation. Therefore, transformations are useful in order to improve the results. (e.g. log(variance), etc.) -->

## The algorithm



# Application

In this section, we will apply the SEM algorithm to a bivariate normal example. To verify our implementation, we will first reproduce the results of @meng1991sem in their bivariate normal example. Afterwards, we will conduct a simulation study to evaluate the quality of the SEM algorithm by comparing the results to the direct estimation of the large-sample observed-data covariance matrix, which is possible to compute in the bivariate normal example, and by performing the simulation for different missing-data mechanisms. Let's first introduce the bivariate normal model.

## A bivariate normal model

We consider an i.i.d. sample of size $n$ from a bivariate normal distribution:

$$
\left(\begin{array}{c} Y_{1,1}\\ Y_{1,2}\\ \end{array} \right), \ldots, \left(\begin{array}{c} Y_{n,1}\\ Y_{n,2}\\ \end{array} \right) \; \overset{\text{i.i.d}}{\sim} \; N\left( \bm \mu = \left(\begin{array}{c} \mu_1\\ \mu_2\\ \end{array} \right), \bm \Sigma = \left(\begin{array}{cc} \sigma_{11} & \sigma_{12}\\ \sigma_{21} & \sigma_{22}\\ \end{array} \right) \right),
$$

where $\bm Y_1 = (Y_{1,1}, \ldots, Y_{n,1})$ is fully observed but from $\bm Y_2 = (Y_{1,2}, \ldots, Y_{n,2})$ we only observe $m < n$ variables and consequently $n-m$ variables are missing. This means that $\bm Y = (\bm Y_1, \bm Y_2) = (\bm \Yobs, \bm \Ymis)$ with

\begin{align*}
\bm \Yobs &= (Y_{1,1}, \ldots, Y_{n,1}, Y_{1,2}, \ldots, Y_{m,2}) \quad \text{ and}\\
\bm \Ymis &= (Y_{m+1,2}, \ldots, Y_{n,2}).
\end{align*}

Since $\sigma_{12} = \sigma_{21}$, the bivariate normal is fully characterized by $\bm \theta = (\mu_1, \mu_2, \sigma_{11}, \sigma_{22}, \sigma_{12})$ and has density $f_1$ is given by

$$
f_1(y_1, y_2 \, | \, \bm \theta) = (2\pi)^{-1} |\bm \Sigma|^{-\frac{1}{2}} \exp\left(- \frac{1}{2} (\bm y - \bm \mu)^T \bm \Sigma^{-1} (\bm y - \bm \mu) \right)
$$
with

$$
|\bm \Sigma| = \sigma_{11} \sigma_{22} - \sigma_{12}^2 \; \text{ and } \; \bm \Sigma^{-1} = |\bm \Sigma|^{-1} \left(\begin{array}{cc} \sigma_{22} & -\sigma_{12}\\ - \sigma_{12} & \sigma_{11}\\ \end{array} \right).
$$


### The observed-data log-Likelihood

Recall that in presence of missing-data, the theoretically correct ML estimate would be the parameter maximizing the (log-)likelihood of the *observed* data. For the sample $\bm \Yobs$, we would have the following observed-data log-likelihood:

\begin{align*}
L(\bm \theta \, | \, \bm \yobs) = &\log f(\bm \yobs \, | \, \bm \theta)\\
= &\sum_{i=1}^n \log f_{\Yobs} (\yobs \, | \, \bm \theta)\\
= &\sum_{i=1}^m \log f_{1} (y_{i1}, y_{i2}  \, | \, \bm \theta) + \sum_{i=m+1}^n \log f_{1} (y_{i1}  \, | \, \mu_1, \sigma_{11})\\
= &-m \log 2 \pi - \frac{m}{2} \log |\bm \Sigma| - \frac{1}{2} \sum_{i=1}^m (\bm y_i - \bm \mu)^T \bm \Sigma^{-1} (\bm y_i - \bm \mu)\\
&-\frac{n-m}{2} \log 2 \pi - \frac{n-m}{2} \log \sigma_{11} - \frac{1}{2 \sigma_{11}} \sum_{i=m+1}^n (y_{i1} - \mu_1)^2
\end{align*}

because for samples $m+1$ to $n$ we only observe $Y_1$. Since for the bivariate normal, we are able to state the asymptotic variance-covariance matrix based on $\bm \Yobs$ directly, this will serve as a benchmark for the SEM algorithm.

### The complete-data log-likelihood

As we have seen, the EM algorithm uses the conditional expectation of the complete-data log-likelihood $Q(\bm \theta \, | \, \bm \theta^{(t)}) = \E[L(\bm \theta \, | \, \bm Y) \, | \, \bm \Yobs, \bm \theta^{(t)}]$ to iteratively approximate the ML estimate of the (log-)likelihood of the observed data. The complete-data log-likelihood of the bivariate normal sample $\bm Y$ is given by

\begin{align*}
L(\bm \theta \, | \, \bm y) = &\log f(\bm y \, | \, \bm \theta)\\
= &\sum_{i=1}^n \log f_1(y_{i1}, y_{i2} \, | \, \bm \theta)\\
= &-n \log 2 \pi - \frac{n}{2} \log |\bm \Sigma| - \frac{1}{2} \sum_{i=1}^n (\bm y_i - \bm \mu)^T \bm \Sigma^{-1} (\bm y_i - \bm \mu)\\
= &-n \log 2 \pi - \frac{n}{2} \log |\bm \Sigma| - \frac{1}{2} |\bm \Sigma|^{-1} \Big \{ \sigma_{22} T_{11} + \sigma_{11} T_{22} - 2 \sigma_{12} T_{12}\\
&- 2 \Big( T_1 (\mu_1 \sigma_{22} - \mu_2 \sigma_{12}) + T_2 (\mu_2 \sigma_{11} - \mu_1 \sigma_{12}) \Big)\\
&+ n (\mu_1^2 \sigma_{22} + \mu_2^2 \sigma_{11} - 2 \mu_1 \mu_2 \sigma_{12}) \Big \}
\end{align*}

since the bivariate normal is an exponential family and therefore the log-likelihood is linear in the sufficient statistics

$$
T_1 = \sum_{i=1}^n y_{i1}, \quad T_2 = \sum_{i=1}^n y_{i2}, \quad T_{11} = \sum_{i=1}^n y_{i1}^2, \quad T_{22} = \sum_{i=1}^n y_{i2}^2, \quad T_{12} = \sum_{i=1}^n y_{i1} y_{i2}.
$$

### E step and M step

As we can see from the complete-data log-likelihood, in order to compute the conditional expectation of the complete-data log-likelihood $Q(\bm \theta \, | \, \bm \theta^{(t)}) = \E[L(\bm \theta \, | \, \bm Y) \, | \, \bm \Yobs = \bm \yobs, \bm \theta^{(t)}]$ for given data $\bm \Yobs = \bm \yobs$ and currently best parameter $\theta^{(t)}$, we just have to compute the conditional expectational values of the sufficient statistics (as we already concluded in section 3.3 for exponential families in general). Since the samples $(Y_{1,1}, Y_{1,2}), \ldots, (Y_{n,1}, Y_{n,2})$ are independent, we only have to evaluate

$$
\E[Y_{i2} \, | \, Y_{i1} = y_{i1}, \bm \theta^{(t)}] \; \text{ and } \; \E[Y_{i2}^2 \, | \, Y_{i1} = y_{i1}, \bm \theta^{(t)}]
$$

for the missing samples $i = m+1, \ldots, n$. Because for the bivariate normal, the conditional distribution is given by

$$
\left( Y_2 \, | \, Y_1 = y_1 \right) \sim N \left(\mu_2 + \frac{\sigma_{12}}{\sigma_{11}}(y_1 - \mu_1), \sigma_{22}(1-\rho^2) \right), \quad \rho = \frac{\sigma_{12}}{\sqrt{\sigma_{11} \sigma_{22}}},
$$

we get the following **E step**:

\begin{align*}
\E[Y_{i2} \, | \, Y_{i1} &= y_{i1}, \bm \theta^{(t)}] = \mu_2^{(t)} + \left(\frac{\sigma_{12}^{(t)}}{\sigma_{11}^{(t)}}\right)(y_{i1} - \mu_1^{(t)}) = \underbrace{\mu_2^{(t)} - \left(\frac{\sigma_{12}^{(t)}}{\sigma_{11}^{(t)}}\right) \mu_1^{(t)}}_{=: \beta_0^{(t)}} + \underbrace{\left(\frac{\sigma_{12}^{(t)}}{\sigma_{11}^{(t)}}\right)}_{=: \beta_1^{(t)}} y_{i1} =: y_{i2}^{(t)}\\
\E[Y_{i2}^2 \, | \, Y_{i1} &= y_{i1}, \bm \theta^{(t)}] = \sigma_{22}^{(t)} (1 - {\rho^{(t)}}^2) + \big(y_{i2}^{(t)}\big)^2
\end{align*}

for the missing values $i = m+1, \ldots, n$ in the $t$-th iteration which is familiar from linear regression ($\hat y = \hat \beta_0 + \hat \beta_1 x$ with $\hat \beta_0 = \bar y - \hat \beta_1 \bar x$). Using the conditional expectational values with the usual complete-data ML estimators

\begin{align*}
\hat \mu_k &= \frac{1}{n} \, T_k , \quad k = 1,2\\
\hat \sigma_{kl} &= \frac{1}{n} \, \left( T_{kl} - \frac{1}{n} T_k T_l \right), \quad k,l = 1,2.
\end{align*}

then gives the parameter updates $\bm \theta^{(t+1)}$ of the **M step**. Note that since there are only missing values for $Y_2$, the parameters of $Y_1$ converge immediately after the first EM iteration with $\hat \mu_1 = \bm{\bar y_1}$ and $\hat \sigma_{11} = T_{11}/n - \bm{\bar y_1}^2$.

### The complete-data Fisher information

Since the bivarate normal is also a *regular* exponential family, in order to evaluate the asymptotic complete-data variance-covariance matrix $V_c = \I_{oc}^{-1}$, we can use the standard Fisher information matrix $\I(\bm \theta)$ as mentioned in section 3.3 because for regular exponential families we have $\I_{oc} = \I(\bm \theta)$. The Fisher information for the bivariate normal can be derived from the general expression of the Fisher information for the multivariate normal distribution.[^2] We get

$$
\I_n(\bm \theta) = n \cdot \I(\bm \theta) = n \cdot |\bm \Sigma|^{-2} \left(\begin{array}{ccccc} 
\sigma_{22} |\bm \Sigma| & \cdots & \cdots & \cdots & \cdots \\
\sigma_{12} |\bm \Sigma| & \sigma_{11} |\bm \Sigma| & \cdots & \cdots & \cdots \\
0 & 0 & \frac{1}{2} \sigma_{22}^2 & \cdots & \cdots \\
0 & 0 & \frac{1}{2} \sigma_{12}^2 & \frac{1}{2} \sigma_{11}^2 & \cdots \\
0 & 0 & -\sigma_{22} \sigma_{12} & -\sigma_{11} \sigma_{12} & \sigma_{11} \sigma_{22} + \sigma_{12}^2
\end{array} \right)
$$

with Inverse

$$
\I_n^{-1}(\bm \theta) = \frac{1}{n} \left(\begin{array}{ccccc} 
\sigma_{11} & \cdots & \cdots & \cdots & \cdots \\
-\sigma_{12} & \sigma_{22} & \cdots & \cdots & \cdots \\
0 & 0 & 2 \sigma_{11}^2 & \cdots & \cdots \\
0 & 0 & \frac{2 |\bm \Sigma|^2 (\sigma_{11} \sigma_{22} \sigma_{12}^2 - \sigma_{12}^4)}{c}  & 2 \sigma_{22}^2 & \cdots \\
0 & 0 & 2\sigma_{11} \sigma_{12} & 2\sigma_{22} \sigma_{12} & \frac{|\bm \Sigma|^2 (\sigma_{11}^2 \sigma_{22}^2 - \sigma_{12}^4)}{c}
\end{array} \right)
$$

where

$$
c := - \sigma_{12}^6 + 3 \sigma_{11} \sigma_{22} \sigma_{12}^4 - 3 (\sigma_{11} \sigma_{22} \sigma_{12})^2 + (\sigma_{11} \sigma_{22})^3.
$$

Now we have all the formulas needed to implement the EM and SEM algorithm for the bivariate normal example.


### No missing information in some components

In a bivariate case, where only one variable contains missing data and the other variable is complete, the calculation of $V$ changes. In this case the parameter $\bm\theta$ for the complete data converges in just one step, indipendent of the initial value. The denominator of (??) will directly be zero and therefore a rate cannot be obtained using SEM. Since we know that $M(\bm\theta)$ for the complete data component is constant, we can directly state that $r_{ij} = 0$ for that component and its calculation is not necessary. Therefore the matrix $DM$ is given by

$$
DM = \begin{pmatrix} 0 & A\\ 0 & DM^* \end{pmatrix},
$$

where $DM^*$ is a quadratic submatrix of size $d_2 \times d_2$ containing the parameter rates for the incomplete data and $A$ is a matrix of size $d_1 \times d_2$. The upper left area is a $d_1 \times d_1$ matrix, where $d_1$ is the dimension of the parameters from the complete data and $d_2$ the dimension of the missing data parameters, where holds that $d_1 + d_2 = d$. We If we assume that $I_{oc}^{-1}$ is given by

$$
I_{oc}^{-1} = \begin{pmatrix} G_1 & G_2\\ G_2^T & G_3 \end{pmatrix},
$$

the variance $V$ is can be stated as

$$
V = I_{oc}^{-1} + \begin{pmatrix} 0 & 0\\ 0 & \Delta V^* \end{pmatrix} = \begin{pmatrix} G_1 & G_2\\ G_2^T & G_3 + \Delta V^* \end{pmatrix}.
$$

The matrix

$$
\Delta V^* = (G_3 - G_2^TG_1^{-1}G_2) DM^*(I_{d_2}-DM^*)^{-1}
$$

is proven in appendix of @meng1991sem, where matrix $A$ is identified with $A = -G_1^{-1} G_2 DM^*$.

## Real data application

In this section we want to introduce the main algorithm for EM and SEM computation. Afterwards the results of @meng1991sem in their bivariate example in section 4.4 will be reproduced to verify, that our implementation runs correctly.

<!-- stopping criterion depending on symmetry of matrix (if matrix is very symmetric, stop em) -->



\newpage

<!--Footnotes-->
[^1]: One can show that $L(\theta^{(t+1)} \, | \, \bm \Yobs) - L(\theta^{(t)} \, | \, \bm \Yobs) \geq 0$ using Jensen's inequality. See for example the original EM paper by @dempster1977maximum for a detailed proof.
[^2]: See for example <https://en.wikipedia.org/wiki/Fisher_information#Multivariate_normal_distribution>.


<!-- References -->
# References
