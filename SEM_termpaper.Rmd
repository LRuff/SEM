---
output: 
  pdf_document:
    fig_caption: true
    keep_tex: true
    latex_engine: pdflatex
    number_sections: true
header-includes:
- \usepackage[english]{babel}
- \usepackage{setspace}
- \hyphenation{}
- \pagenumbering{arabic}
- \newcommand{\Yobs}{Y_{\text{obs}}}
- \newcommand{\Ymis}{Y_{\text{mis}}}
- \newcommand{\E}{\mathbb{E}}
- \newcommand{\argmax}{\text{argmax}}
- \newcommand{\I}{\mathcal{I}}
fontsize: 10pt
geometry: a4paper, left=30mm, right=30mm, top=20mm, bottom=20mm, includefoot
bibliography: references.bib
documentclass: article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<!-- Titlepage -->
\thispagestyle{empty}

\begin{raggedright}
  Freie Universit√§t Berlin\\
  Chair of Statistics\\
  Location: Berlin\\
  Winter Term 2016/2017\\
  Lecture: Missing Data Analysis\\
  Examiner: Dr. Florian Meinfelder
\end{raggedright}

\vspace*{\fill}
  \begin{center}
    \textbf{\Large{The Supplemented EM algorithm}}%
  \end{center}
\vfill

\begin{raggedleft}
  Carlo Michaelis (5043128), \href{mailto:carlo.michaelis@gmail.com}{carlo.michaelis@gmail.com}\\
  Lukas Ruff (5029054), \href{mailto:contact@lukasruff.com}{contact@lukasruff.com}\\
  \ \\
  Master Statistics\\
  \today\\
\end{raggedleft}

\newpage


<!-- Table of Contents -->
\tableofcontents
\newpage


<!-- List of Figures and Tables -->
\listoffigures
\listoftables
\newpage


<!-- Document Body -->
# Introduction

general missing data context

motivation for em

The expectation maximization (EM) algorithm was devloped by @dempster1977maximum to obtain maximum likelihood estimations when the underlying distribution depends on unobserved variables.

em and vaiance estimation in em

list of approaches for variance estimation in em

pros/cons of bootstrap and multiple imputation (comparison to other methods)

@enders2010missing,

# The EM Algorithm

The EM algorithm consists of two steps, namely the *expectation step* and the *maximization step*. Both steps are applied alternately until some convergence criterion is reached. Depending on given data structure, especially the missing data, we can find a *rate of convergence* $DM$ for the parameters. Both aspects of the EM algorithm are explained in this section.

## Objective and algorithm

The model for the complete data is given by

$$
f(Y|\theta),\quad \theta = (\theta_1, ..., \theta_d)
$$

and $Y$ can be separated into a observed $\Yobs$ and a missed part $\Ymis$, such that $Y = (\Yobs, \Ymis)$. We define $\theta^*$ as the maximum likelihood estima (MLE) of the parameter $\theta$. The estimation is based on the observed data $\Yobs$ and therefore maximizes $f(\Yobs|\theta)$. The algorithm starts with an initial parameter value $\theta^{(0)}$, which becomes $\theta^{(t)}$ at the `t`'hs EM iteration, which converges to $\theta^*$.

<!-- explain objective of em, how to obtain Q -->

**Expectation step**. The idea of the so called expectation step (*E step*) is to obtain the expectation value of the log-likelihood function related to the missing data, given the observed data and the parameter at iteration `t`, which is given by

$$
Q(\theta|\theta^{(t)}) = \E_{\Ymis|\Yobs,\theta^{(t)}}[L(\theta|Y)]  = \E_{\Ymis|\Yobs,\theta^{(t)}}[\ln f(Y|\theta)],
$$

while the objective can be calculated by

$$
\E_{\Ymis|Yobs,\theta^{(t)}}[L(\theta|Y)] = \int L(\theta|Y) f(\Ymis|\Yobs,\theta=\theta^{(t)})\,d\Ymis.
$$

**Minimization step**. In the Minimization step (*M step*) the objective of the E step will be maximized, which means that

$$
Q(\theta^{(t+1)}|\theta^{(t)}) \ge Q(\theta|\theta^{(t)}), \quad \forall \theta.
$$

More clearly the parameter $\theta^{(t+1)}$ is obtained by 

$$
\theta^{(t+1)} = \underset{\theta}{\argmax} \,Q(\theta|\theta^{(t)}).
$$

It means that we search for $\theta^{(t+1)}$ which maximizes the objective, given $\theta^{(t)}$ and we can now search for $Q(\theta|\theta^{(t+1)})$ using the E step, which starts the next iteration.

<!-- In box: Application to regression model, normal case -->

## Rate of convergence

The EM algorithm updates the parameter in every iteration of E step and M step from $\theta^{(t)}$ to $\theta^{(t+1)}$. To describe this behavior we can define a mapping

$$
M: \Theta \to \Theta,\; \theta \mapsto M(\theta)\quad \text{such that}\quad \theta^{(t+1)} = M(\theta^{(t)}),
$$

where $\Theta$ is the parameter space. While $\theta^{(t)}$ converges to $\theta^*$, we can state

$$
\theta^* = M(\theta^*),
$$

where $\theta^*$ is a fixed point of $M$.

To get a rate of convergence, we need a derivation of $M$, which we call $DM$. Therefore we do a taylor approximation of $\theta^{(t+1)}$ in the neighborhood of $\theta^*$ and stop after the linear term, which results in

\begin{align*}
\theta^{(t+1)} &= M(\theta^*) + M'(\theta^*) (\theta^{(t+1)} - \theta^*) + \frac{1}{2} M''(\theta^*)(\theta^{(t+1)} - \theta^*)^2 + ...\\
&\approx \theta^* + M'(\theta^*) (\theta^{(t+1)} - \theta^*).
\end{align*}

Rearraning the equation finally gives

$$
M'(\theta^*) = DM \approx \frac{\theta^{(t+1)} - \theta^*}{\theta^{(t)} - \theta^*},\quad \text{where}\quad DM = \left( \frac{\partial M_j(\theta)}{\partial \theta} \right) \big|_{\theta = \theta^*}.
$$

We call $DM$ the *rate matrix*.

<!-- describe mapping as combination of e-step, m-step -->
<!-- attributes of m -> fixed points -->
<!-- taylor -->
<!-- link later used -->
<!-- In box: Application to regression model, normal case -->

<!-- Short argument, why maximizing Q leads to a better parameter estimation which has a higher *observed-data* (log-)likelihood. Simple proof using Jensen! -->
<!-- The EM algorithm converges linearly with rate matrix DM -->

# The SEM algorithm

In comparison to multiple imputation methods, asymptotic variance-covariance matrices of the parameter estimaton (e.g. standard errors) are not given automatically when using the EM algorithm. Instead, additional steps are required in order to compute standard errors of the estimates. One approach introduced by @meng1991sem is the so-called *supplemented EM* or *SEM* algorithm. This procedure obtains numerically stable asymptotic variance-covariance matrices only from computing the complete-data asymptotic variance-covariance matrix, using code for the E and M steps of EM, and standard matrix operations. The key idea of the SEM algorithm is effectively to numerically differentiate the implicit mapping $\theta \to M(\theta)$ defined by the EM algorithm, i.e. defining a method to numerically approximate $DM$.


## Asymptotic variance-covariance matrix of $(\theta - \hat \theta)$ based on $\Yobs$

The desired asymptotic variance-covariance matrix of $(\theta - \hat \theta)$ based on $\Yobs$ is given by the inverse of the observed information matrix, i.e.

$$
V = \I_{o}^{-1}(\hat \theta \, | \, \Yobs),
$$

where the observed information matrix $\I_{o}(\theta \, | \, \Yobs)$ is the negative second derivative of the log-likelihood of the observed data,

\begin{equation}
\label{eq:Iobs}
\I_{o}(\theta \, | \, \Yobs) = -\frac{\partial^2 \log f(\Yobs \, | \, \theta)}{\partial\theta \, \partial\theta}.
\end{equation}

The problem is that it can often be very difficult to evaluate (\ref{eq:Iobs}) directly. Therefore, another representation of $V$ is preferred. Precisely, we will derive the following representation of $V$:

\begin{equation}
\label{eq:V}
V = \I_{oc}^{-1} + \Delta V, 
\end{equation}

where

$$
\I_{oc} = \E[\I_{o}(\theta \, | \, Y) \, | \, \Yobs, \theta ] \bigg\rvert_{\theta = \hat \theta}
$$

is the conditional expectation w.r.t. $\Yobs$ of the complete-data observed information matrix,

$$
\I_{o}(\theta \, | \, Y) = -\frac{\partial^2 \log f(Y \, | \, \theta)}{\partial\theta \, \partial\theta},
$$

evaluated at $\theta = \hat \theta$, and

$$
\Delta V =  \I_{oc}^{-1} \, DM \, (I_d - DM)^{-1},
$$

where $DM$ is the rate matix from (...) and $I_d$ is the $d$-dimensional identity matrix.

<!-- TODO: How is the (well-known) asymptotic variance V derived? -->
<!-- explain why only large sample i.e. asymptotic -->
<!-- Explain relation to fisher information matrix (regularity / large sample) -->

## Proof that $V = \I_{oc}^{-1} + \Delta V$

From Bayes' rule on densities, we have the factorization

$$
f(Y \, | \, \theta) = f(\Yobs \, | \, \theta) \, f(\Ymis \, | \, \Yobs, \theta),
$$

and therefore it follows for the log-likelihood of $\theta$ given $\Yobs$

\begin{align*}
L(\theta \, | \, \Yobs) &= \log f(\Yobs \, | \, \theta)\\
&= \log f(Y \, | \, \theta) - \log f(\Ymis \, | \, \Yobs, \theta) = L(\theta \, | \, Y) - \log f(\Ymis \, | \, \Yobs, \theta).
\end{align*}

Taking second derivatives and the expectation over the conditional distribution $f(\Ymis \, | \, \Yobs , \theta)$, we get that

\begin{align*}
&&\E\left[\frac{\partial^2 L(\theta \, | \, \Yobs)}{\partial\theta \, \partial\theta} \, | \, \Yobs, \theta \right] &= \E\left[\frac{\partial^2 L(\theta \, | \, Y)}{\partial\theta \, \partial\theta} \, | \, \Yobs, \theta \right] - \E\left[\frac{\partial^2 \log f(\Ymis \, | \, \Yobs, \theta)}{\partial\theta \, \partial\theta} \, | \, \Yobs, \theta \right]\\
\iff &&-\frac{\partial^2 L(\theta \, | \, \Yobs)}{\partial\theta \, \partial\theta} &= \E\left[-\frac{\partial^2 L(\theta \, | \, Y)}{\partial\theta \, \partial\theta} \, | \, \Yobs, \theta \right] - \E\left[-\frac{\partial^2 \log f(\Ymis \, | \, \Yobs, \theta)}{\partial\theta \, \partial\theta} \, | \, \Yobs, \theta \right] \\
\iff &&\I_o(\theta \, | \, \Yobs) &= \E\left[\I_o(\theta \, | \, Y) \, | \, \Yobs, \theta \right] - \E\left[-\frac{\partial^2 \log f(\Ymis \, | \, \Yobs, \theta)}{\partial\theta \, \partial\theta} \, | \, \Yobs, \theta \right].
\end{align*}

Evaluating this equation at $\theta = \hat \theta$ gives

\begin{equation}
\label{eq:informationprinciple}
\I_o(\hat \theta \, | \, \Yobs) = \I_{oc} - \I_{om},
\end{equation}

where 

$$
\I_{om} = \left. \E\left[-\frac{\partial^2 \log f(\Ymis \, | \, \Yobs, \theta)}{\partial\theta \, \partial\theta} \, | \, \Yobs, \theta \right] \right\rvert_{\theta = \hat \theta}
$$

can be viewed as the missing information. Equation (\ref{eq:informationprinciple}) is also called *missing information principle* with the nice interpretation

$$
\textit{observed information} = \textit{complete information } - \textit{missing information}.
$$

Besides, @dempster1977maximum showed that when maximizing $Q(\theta \, | \, \theta^{(t)})$ in the M step, i.e. when setting the first derivative equal to zero, it holds that

$$
DM = \I_{om} \I_{oc}^{-1}.
$$

This can be used in (\ref{eq:informationprinciple}) to obtain

$$
\I_o(\hat \theta \, | \, \Yobs) = \I_{oc} - \I_{om} = (I_d - \I_{om}\I_{oc}^{-1})\I_{oc} = (I_d - DM)\I_{oc}.
$$

Finally, by inverting both sides of the equation, we get

\begin{align*}
V &= \I_o^{-1}(\hat \theta \, | \, \Yobs)\\
&= \I_{oc}^{-1}(I_d - DM)^{-1}\\
&= \I_{oc}^{-1}(I_d - DM + DM)(I_d - DM)^{-1}\\
&= \I_{oc}^{-1} + \I_{oc}^{-1} DM (I_d - DM)^{-1} = \I_{oc}^{-1} + \Delta V.
\end{align*}


<!-- proof 2.4.5 optional -->

transition: Evaluation of V (1) Ioc (2) DM (3) V

## Ioc for exponential family

exponential family

regularity of expfam (link to fisher information, see above)

<!-- Optional: What to do if not regular? -->

In box: Application to regression model, normal case

## DM

r_ij component

component wise derivation of matrix

(1) EM to obtain theta (2) SEM to obtain r_ij

<!-- OPtional: Alternatives for approximation of DM -->


<!-- ## Notes -->
<!-- It is important to emphasize that the variance-covariance matrix obtained from SEM is based on the second derivatives of the observed-data log-likelihood and only is inferentially valid asymptotically, i.e. for large samples. -->
<!-- The more normal the likelihood, the better the approximation. Therefore, transformations are useful in order to improve the results. (e.g. log(variance), etc.) -->
<!-- Switch notation from $\theta^*$ to $\hat \theta$? This is more consistent with general statistical notation... -->


# Application

stopping criterion depending on symmetry of matrix (if matrix is very symmetric, stop em)

\newpage


<!-- References -->
# References
