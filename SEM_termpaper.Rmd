---
output: 
  pdf_document:
    fig_caption: true
    keep_tex: true
    latex_engine: pdflatex
    number_sections: true
header-includes:
- \usepackage[english]{babel}
- \usepackage{setspace}
- \usepackage{bm}
- \hyphenation{}
- \pagenumbering{arabic}
- \numberwithin{equation}{section}
- \newcommand{\Yobs}{Y_{\text{obs}}}
- \newcommand{\Ymis}{Y_{\text{mis}}}
- \newcommand{\E}{\mathbb{E}}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\argmax}{\text{argmax}}
- \newcommand{\I}{\mathcal{I}}
fontsize: 10pt
geometry: a4paper, left=30mm, right=30mm, top=20mm, bottom=20mm, includefoot
bibliography: references.bib
documentclass: article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<!-- Titlepage -->
\thispagestyle{empty}

\begin{raggedright}
  Freie Universit√§t Berlin\\
  Chair of Statistics\\
  Location: Berlin\\
  Winter Term 2016/2017\\
  Lecture: Missing Data Analysis\\
  Examiner: Dr. Florian Meinfelder
\end{raggedright}

\vspace*{\fill}
  \begin{center}
    \textbf{\Large{The Supplemented EM algorithm}}%
  \end{center}
\vfill

\begin{raggedleft}
  Carlo Michaelis (5043128), \href{mailto:carlo.michaelis@gmail.com}{carlo.michaelis@gmail.com}\\
  Lukas Ruff (5029054), \href{mailto:contact@lukasruff.com}{contact@lukasruff.com}\\
  \ \\
  Master Statistics\\
  \today\\
\end{raggedleft}

\newpage


<!-- Table of Contents -->
\tableofcontents
\newpage


<!-- List of Figures and Tables -->
\listoffigures
\listoftables
\newpage


<!-- Document Body -->
# Introduction

There are several methods to deal with missing data. Beside multiple imputation it is possible to estimate parameters by using the likelihood function. Unfortunatly in many cases, solving the likelihood of the observed data is not an easytask. Therefore the so called expectation maximization (EM) algorithm can be used. It was developed by @dempster1977maximum to obtain maximum likelihood estimations when the underlying distribution depends on unobserved variables.

While the algortihm itself - finding parameters $\hat\theta$ - is feasible, the estimtaion of the variance of the estimators (e.g. to obtain standard errors) is somehow difficult. Different approaches were suggested to face this problem:

 * Algebraic analysis [@meilijson1989fast]
 * Monte Carlo evaluation using multiple imputation [@rubin2004multiple]
 * Numerical methods [@carlin1987seasonal]
 * Resampling: Bootstrap/Jackknife
 * Supplemented EM (SEM)

While methods like resampling or algebraic analysis mostly depends on iid data, SEM can be done without this assumption. Furthermore numerical methods try to calculate the variance covariance matrix directly, while SEM just computes the increase of variance, which is computational more stable. Lastly Monte Carlo evaluation need more extra code and the result depends on the number of imputations. In 1990s, when the SEM was developped by @meng1991sem, the computational power was far less than today, where it was not possible to do as many imputation as are possible today.

In this term paper the goal is to describe the EM and SEM algorithm. The theory is done in general and afterwards applied to a bivariate normal problem. Furthermore we computed an example in `R` regarding the bivariate problem, which should give an intuition for the EM and the SEM algorithm.


# A Recap of the EM Algorithm

The Expectation Maximization (EM) algorithm is an iterative procedure to maximize the likelihood of a model with missing or latent variables. It consists of two steps, namely the *expectation step* and the *maximization step*. Both steps are applied alternately until some convergence criterion is reached. Depending on the given data structure, especially the missing data, we can find a *rate of convergence* $DM$ for the parameters. Both aspects of the EM algorithm are explained in this section.

## Objective and algorithm

Let the model of the complete data $\bm Y$, which has an observed part $\bm \Yobs$ and missing values $\bm \Ymis$, that is $\bm Y = (\bm \Yobs, \bm \Ymis)$, be described by the complete-data density $f(\bm Y \, | \, \bm \theta)$, $\bm \theta = (\theta_1, ..., \theta_d)$, and denote the complete-data log-likelihood by

$$
L(\bm \theta \, | \, \bm Y) = \log f(\bm Y \, | \, \bm \theta),
$$

that is we assume the missing-data mechanism to be ignorable. The EM algorithm finds the maximum likelihood estimate (MLE) of the parameter $\bm \theta$ of the *observed* likelihood $f(\bm \Yobs \, | \, \bm \theta)$, which we define by $\bm{\hat \theta}$. The key feature of the EM algorithm is, that this maximization of the observed (log-)likelihood is not carried out directly, but indirectly and iteratively by maximizing the conditional expectation over $f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta^{(t)})$ of the complete-data log-likelihood $L(\bm \theta \, | \, \bm Y)$, where $\bm \theta^{(t)}$ is the parameter value at the $t$-th iteration. The algorithm starts with some initial parameter value $\bm \theta^{(0)}$.

**Expectation step**. The expectation step (*E step*) computes the conditional expectation of the complete-data log-likelihood over $f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta^{(t)})$ using the parameter of iteration $t$, that is

$$
Q(\bm \theta \, | \, \bm \theta^{(t)}) = \E[L(\bm \theta \, | \, \bm Y) \, | \, \bm \Yobs, \bm \theta^{(t)}]  = \int L(\bm \theta \, | \, \bm Y) f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta^{(t)}) \, d\bm \Ymis.
$$

Therefore, the E step estimates the missing data parts of the complete-data log-likelihood by the expectational value using the observed data and the current best parameter estimates $\bm \theta^{(t)}$ of the model.


**Maximization step**. In the maximization step (*M step*) the objective of the E step is maximized in $\bm \theta$ in order to obtain an update of the model parameters, which means solving

$$
\bm \theta^{(t+1)} = \underset{\bm \theta}{\argmax} \,Q(\bm \theta \, | \, \bm \theta^{(t)}), \qquad \text{ i.e. } \, Q(\bm \theta^{(t+1)} \, | \, \bm \theta^{(t)}) \geq Q(\bm \theta \, | \, \bm \theta^{(t)}), \quad \forall \bm \theta.
$$

The updated parameters $\bm \theta^{(t+1)}$ are always better in the sense, that they have a greater or equal observed-data (log-)likelihood than the previous parameters $\bm \theta^{(t+1)}$, that is $L(\bm \theta^{(t+1)} \, | \, \bm \Yobs) \geq L(\bm \theta^{(t)} \, | \, \bm \Yobs)$.[^1]
After obtaining the parameter update $\bm \theta^{(t+1)}$, it can be used in the following E and M step which conduct the next EM iteration.  

For the large class of exponential families, the EM algorithm is usually easy to implement: The M step is identical to the computation of ML estimates from the complete-data log-likelihood $L(\bm \theta \, | \, \bm Y)$; the E step for most problems only involves calculating the conditional expectation of the sufficient statistics, which are easy to compute from the moments of the conditional distribution.

## Rate of convergence

The EM algorithm updates the parameter in every iteration of E step and M step from $\bm \theta^{(t)}$ to $\bm \theta^{(t+1)}$. To describe this behavior we can define a mapping

$$
M: \Theta \to \Theta,\; \bm \theta \mapsto M(\bm \theta)\quad \text{such that}\quad M(\bm \theta^{(t)}) = \bm \theta^{(t+1)}, \, t = 0, 1, \ldots \;
$$

where $\Theta$ is the parameter space. If $\left(\bm \theta^{(t)}\right)$ converges to $\bm{\hat \theta}$ and $M$ is continuous, we have

$$
M(\bm{\hat \theta}) = \bm{\hat \theta},
$$

that is $\bm{\hat \theta}$ is a fixed point of $M$. Thus, in the neighborhood of $\bm{\hat \theta}$, we get 

\begin{align*}
M(\bm \theta^{(t)}) &\approx M(\bm{\hat \theta}) + DM (\bm \theta^{(t)} - \bm{\hat \theta})\\
\iff \qquad \bm \theta^{(t+1)} &\approx \bm{\hat \theta} + DM (\bm \theta^{(t)} - \bm{\hat \theta})\\
\iff \, \bm \theta^{(t+1)} - \bm{\hat \theta} &\approx DM (\bm \theta^{(t)} - \bm{\hat \theta})
\end{align*}

doing a taylor approximation of $M(\bm \theta^{(t)})$ at $\bm{\hat \theta}$ stopping after the linear term, where

\begin{equation}
\label{eq:rate}
DM = \left.\left( \frac{\partial M_j(\bm \theta)}{\partial \theta_i} \right) \right|_{\bm \theta = \bm{\hat \theta}}
\end{equation}

is the $d \times d$ Jacobian matrix of $M(\bm \theta) = (M_1(\bm \theta), \ldots, M_d(\bm \theta))$ evaluated at $\bm{\hat \theta}$. Hence, in the neighborhood of $\bm{\hat \theta}$, the EM algorithm converges linearly with rate matrix $DM$.


# The SEM algorithm

In comparison to multiple imputation methods, asymptotic variance-covariance matrices of the parameter estimaton $\bm{\hat\theta}$ (e.g. standard errors) are not given automatically when using the EM algorithm. Instead, additional steps are required in order to compute standard errors of the estimates. One approach introduced by @meng1991sem is the so-called *supplemented EM* or *SEM* algorithm. This procedure obtains numerically stable asymptotic variance-covariance matrices only from computing the complete-data asymptotic variance-covariance matrix, using code for the E and M steps of EM, and standard matrix operations. The key idea of the SEM algorithm is effectively to numerically differentiate the implicit mapping $\bm \theta \mapsto M(\bm \theta)$ defined by the EM algorithm, i.e. defining a method to numerically approximate the Jacobian matrix $DM$.


## Asymptotic variance-covariance matrix of $(\bm \theta - \bm{\hat \theta})$ based on $\bm \Yobs$

The desired asymptotic variance-covariance matrix of $(\bm \theta - \bm{\hat \theta})$ based on $\bm \Yobs$ is given by the inverse of the observed information matrix, i.e.

$$
V = \I_{o}^{-1}(\bm{\hat \theta} \, | \, \bm \Yobs),
$$

where the observed information matrix $\I_{o}(\bm \theta \, | \, \bm \Yobs)$ is the negative Hessian of the log-likelihood of the observed data,

\begin{equation}
\label{eq:Iobs}
\I_{o}(\bm \theta \, | \, \bm \Yobs) = -\frac{\partial^2 \log f(\bm \Yobs \, | \, \bm \theta)}{\partial\bm \theta \cdot \partial \bm \theta}.
\end{equation}

The problem is that it can often be very difficult to evaluate (\ref{eq:Iobs}) directly. Therefore, another representation of $V$ is preferred. Precisely, we will derive the following decomposition of $V$:

\begin{equation}
\label{eq:V}
V = V_c + \Delta V \qquad \text{ with } \, V_c = \I_{oc}^{-1},
\end{equation}

where $V_c$ is the asymptotic complete-data variance-covariance matrix given by the inverse of the conditional expectation over $f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta)$ of the complete-data information matrix evaluated at $\bm \theta = \bm{\hat \theta}$, that is

\begin{equation}
\label{eq:Ioc}
\I_{oc} = \E[\I_{c}(\bm \theta \, | \, \bm Y) \, | \, \bm \Yobs, \bm \theta ] \bigg\rvert_{\bm \theta = \bm{\hat \theta}} \; , \qquad \I_{c}(\bm \theta \, | \, \bm Y) = -\frac{\partial^2 \log f(\bm Y \, | \, \bm \theta)}{\partial \bm \theta \cdot \partial \bm \theta},
\end{equation}

and $\Delta V$, the increase due to missing data, is given by

\begin{equation}
\label{eq:DeltaV}
\Delta V =  V_c \, DM \, (I_d - DM)^{-1},
\end{equation}

where $DM$ is the rate matix from (\ref{eq:rate}) and $I_d$ is the $d$-dimensional identity matrix. We will now show, why the decomposition in (\ref{eq:V}) holds.

<!-- TODO: How is the (well-known) asymptotic variance V derived? -->
<!-- explain why only large sample i.e. asymptotic -->
<!-- Explain relation to fisher information matrix (regularity / large sample) -->

## Proof that $V = V_c + \Delta V$

From Bayes' rule on densities, we have the factorization

$$
f(\bm Y \, | \, \bm \theta) = f(\bm \Yobs \, | \, \bm \theta) \, f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta),
$$

and therefore it follows for the log-likelihood of $\bm \theta$ given $\bm \Yobs$

\begin{align*}
L(\bm \theta \, | \, \bm \Yobs) &= \log f(\bm \Yobs \, | \, \bm \theta)\\
&= \log f(\bm Y \, | \, \bm \theta) - \log f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta) = L(\bm \theta \, | \, \bm Y) - \log f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta).
\end{align*}

Taking second derivatives and the expectation over the conditional distribution $f(\bm \Ymis \, | \, \bm \Yobs , \bm \theta)$, we further get that

\begin{align*}
&&\E\left[\frac{\partial^2 L(\bm \theta \, | \, \bm \Yobs)}{\partial \bm \theta \cdot \partial \bm \theta} \, | \, \bm \Yobs, \bm \theta \right] &= \E\left[\frac{\partial^2 L(\bm \theta \, | \, \bm Y)}{\partial \bm \theta \cdot \partial \bm \theta} \, | \, \bm \Yobs, \bm \theta \right] - \E\left[\frac{\partial^2 \log f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta)}{\partial \bm \theta \cdot \partial \bm \theta} \, | \, \bm \Yobs, \bm \theta \right]\\
\iff &&-\frac{\partial^2 L(\bm \theta \, | \, \bm \Yobs)}{\partial \bm \theta \cdot \partial \bm \theta} &= \E\left[-\frac{\partial^2 L(\bm \theta \, | \, \bm Y)}{\partial \bm \theta \cdot \partial \bm \theta} \, | \, \bm \Yobs, \bm \theta \right] - \E\left[-\frac{\partial^2 \log f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta)}{\partial \bm \theta \cdot \partial \bm \theta} \, | \, \bm \Yobs, \bm \theta \right] \\
\iff &&\I_o(\bm \theta \, | \, \bm \Yobs) &= \E\left[\I_c(\bm \theta \, | \, \bm Y) \, | \, \bm \Yobs, \bm \theta \right] - \E\left[-\frac{\partial^2 \log f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta)}{\partial\bm \theta \cdot \partial\bm \theta} \, | \, \bm \Yobs, \bm \theta \right].
\end{align*}

Evaluating this equation at $\bm \theta = \bm{\hat \theta}$ gives

\begin{equation}
\label{eq:informationprinciple}
\I_o(\bm{\hat \theta} \, | \, \bm \Yobs) = \I_{oc} - \I_{om},
\end{equation}

where 

$$
\I_{om} = \left. \E\left[-\frac{\partial^2 \log f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta)}{\partial\bm \theta \cdot \partial\bm \theta} \, | \, \bm \Yobs, \bm \theta \right] \right\rvert_{\bm \theta = \bm{\hat \theta}}
$$

can be viewed as the missing information. Equation (\ref{eq:informationprinciple}) is also called *missing information principle* with the nice interpretation

$$
\textit{observed information} = \textit{complete information } - \textit{missing information}.
$$

Besides, @dempster1977maximum showed that when maximizing $Q(\bm \theta \, | \, \bm \theta^{(t)})$ in the M step of the EM algorithm, i.e. when setting the first derivative of $Q(\bm \theta \, | \, \bm \theta^{(t)})$ w.r.t. $\bm \theta$ equal to zero, it holds that

$$
DM = \I_{om} \I_{oc}^{-1}.
$$

This can be used with (\ref{eq:informationprinciple}) to obtain

$$
\I_o(\bm{\hat \theta} \, | \, \bm \Yobs) = \I_{oc} - \I_{om} = (I_d - \I_{om}\I_{oc}^{-1})\I_{oc} = (I_d - DM)\I_{oc}.
$$

Finally, by inverting both sides of this equation, we get the decomposition of the asymptotic variance-covariance matrix based on $\bm \Yobs$:

\begin{align*}
V = \I_o^{-1}(\bm{\hat \theta} \, | \, \bm \Yobs) &= \I_{oc}^{-1}(I_d - DM)^{-1}\\
&= \I_{oc}^{-1}(I_d - DM + DM)(I_d - DM)^{-1}\\
&= \I_{oc}^{-1} + \I_{oc}^{-1} DM (I_d - DM)^{-1}\\
&= \I_{oc}^{-1} + \Delta V = V_c + \Delta V.
\end{align*}

This representation of $V$ is the ansatz of the SEM algorithm, which consists of three parts: (1) the evaluation of $V_c = \I_{oc}^{-1}$, (2) the numerical approximation of $DM$, and (3) using (\ref{eq:V}) with (\ref{eq:DeltaV}) to estimate $V$. We look into the evaluation of $V_c = \I_{oc}^{-1}$ in the next subsection.

<!-- Optional TODO: proof 2.4.5 -->


## Evaluation of $V_c$ for exponential families

We consider the evaluation of $V_c = \I_{oc}^{-1}$ for the case that the complete-data density $f(\bm Y \, | \, \bm \theta)$ is from an exponential family, since this is most common in practical applications. This means that the complete-data density has the following representation

$$
f(\bm Y \, | \, \bm \theta) = h(\bm Y) \exp\{\eta(\bm \theta)^T T(\bm Y) - A(\bm \theta)\} = h(\bm Y) \exp\left\{\sum_{i=1}^s\eta_i(\bm \theta) T_i(\bm Y) - A(\bm \theta) \right\},
$$

where the *natural parameter* $\eta(\bm \theta)$ and the *sufficient statistics* $T(\bm Y)$ are vector functions in $\R^s, s \geq d,$ and the *base measure* $h(\bm Y)$ as well as the *log-normalizer* $A(\bm \theta)$ are scalar functions.

The sufficient statistics $T(\bm Y)$ is crucial for the implementation and application of the EM and SEM algorithm for exponential families. This is because we have

$$
L(\bm \theta \, | \, \bm Y) = \log f(\bm Y \, | \, \bm \theta) = \log h(\bm Y) + \eta(\bm \theta)^T T(\bm Y) - A(\bm \theta)
$$

which means that the complete-data log-likelihood is linear in the sufficient statistics $T(\bm Y)$, i.e. $L(\bm \theta \, | \, \bm Y) = L(\bm \theta \, | \, T(\bm Y))$. This means that in order to compute the E step for exponential families, we practically just have to evaluate the conditional expectation of the sufficient statistics $\E[T(\bm Y) \, | \, \bm \Yobs, \bm \theta^{(t)}]$. Furthermore, linearity in the log-likelihood implies that the complete-data information is also linear in $T(\bm Y)$, that is $\I_{c}(\bm \theta \, | \, \bm Y) = \I_{c}(\bm \theta \, | \, T(\bm Y))$. Therefore, from the definition of $\I_{oc}$ in (\ref{eq:Ioc}), we get that

\begin{align*}
\I_{oc} &= \E[\I_{c}(\bm \theta \, | \, \bm Y) \, | \, \bm \Yobs, \bm \theta ] \bigg\rvert_{\bm \theta = \bm{\hat \theta}}\\
&= \I_{c}(\bm{\hat \theta} \, | \, T^*(\bm \Yobs)),
\end{align*}

by linearity in $T(\bm Y)$ with $T^*(\bm \Yobs) = \E[T(\bm Y) \, | \, \bm \Yobs, \bm{ \hat \theta}]$, which is obtained in the very last E step of the EM procedure. Hence, we can obtain $V_c = \I_{oc}^{-1}$ simply by plugging in the conditional expectation of the sufficient statistics $T^*(\bm \Yobs)$ computed in the very last E step into the inverse of $\I_{c}(\bm \theta \, | \, T(\bm Y))$ evaluated at $\bm \theta = \bm{\hat \theta}$.

When the exponential family is regular (i.e. when $s = d$ and the Jacobian of $\eta(\bm \theta)$ has full rank) the evaluation of $V_c = \I_{oc}^{-1}$ is even simpler. In this case, the complete-data information matrix $\I_{oc}$ is just the standard Fisher information matrix evaluated in $\bm{\hat \theta}$, i.e.

$$
\I_{oc} = \I(\bm{\hat \theta}), \qquad \I(\bm \theta) = \E[\I_c(\bm \theta \, | \, \bm Y) \, | \, \bm \theta].
$$

Inverting the Fisher information then gives $V_c$.

For the case when the complete-data density $f(\bm Y \, | \, \theta)$ is not from an exponential family, the complete-data log-likelihood $L(\bm \theta \, | \, \bm Y)$ is no longer linear in the sufficient statistics $T(\bm Y)$. This can make the evaluation of the E step difficult, because then $Q(\bm \theta \, | \, \bm \theta^{(t)})$ is generally not of closed form. One approach to tackle this problem is to linearize $L(\bm \theta \, | \, \bm Y)$ in the sufficient statistics using a Taylor expansion. See @dempster1977maximum section 3.2 for further details.


## Numerical Approximation of DM

r_ij component

component wise derivation of matrix

(1) EM to obtain theta (2) SEM to obtain r_ij

<!-- Optional: Alternatives for approximation of DM -->


<!-- ## Notes -->
<!-- It is important to emphasize that the variance-covariance matrix obtained from SEM is based on the second derivatives of the observed-data log-likelihood and only is inferentially valid asymptotically, i.e. for large samples. -->
<!-- The more normal the likelihood, the better the approximation. Therefore, transformations are useful in order to improve the results. (e.g. log(variance), etc.) -->

## The algorithm



# Application

<!-- stopping criterion depending on symmetry of matrix (if matrix is very symmetric, stop em) -->

## Bivariate normal

### No missing information in some components

In a bivariate case, where only one variable contains missing data and the other variable is complete, the calculation of $V$ changes. In this case the parameter $\bm\theta$ for the complete data converges in just one step, indipendent of the initial value. The denominator of (??) will directly be zero and therefore a rate cannot be obtained using SEM. Since we know that $M(\bm\theta)$ for the complete data component is constant, we can directly state that $r_{ij} = 0$ for that component and its calculation is not necessary. Therefore the matrix $DM$ is given by

$$
DM = \begin{pmatrix} 0 & A\\ 0 & DM^* \end{pmatrix},
$$

where $DM^*$ is a quadratic submatrix of size $d_2 \times d_2$ containing the parameter rates for the incomplete data and $A$ is a matrix of size $d_1 \times d_2$. The upper left area is a $d_1 \times d_1$ matrix, where $d_1$ is the dimension of the parameters from the complete data and $d_2$ the dimension of the missing data parameters, where holds that $d_1 + d_2 = d$. We If we assume that $I_{oc}^{-1}$ is given by

$$
I_{oc}^{-1} = \begin{pmatrix} G_1 & G_2\\ G_2^T & G_3 \end{pmatrix},
$$

the variance $V$ is can be stated as

$$
V = I_{oc}^{-1} + \begin{pmatrix} 0 & 0\\ 0 & \Delta V^* \end{pmatrix} = \begin{pmatrix} G_1 & G_2\\ G_2^T & G_3 + \Delta V^* \end{pmatrix}.
$$

The matrix

$$
\Delta V^* = (G_3 - G_2^TG_1^{-1}G_2) DM^*(I_{d_2}-DM^*)^{-1}
$$

is proven in appendix of @meng1991sem, where matrix $A$ is identified with $A = -G_1^{-1} G_2 DM^*$.

## Real data application

In this section we want to introduce the main algorithm for EM and SEM computation. Afterwards the results of @meng1991sem in their bivariate example in section 4.4 will be reproduced to verify, that our implementation runs correctly.





\newpage

<!--Footnotes-->
[^1]: One can show that $L(\theta^{(t+1)} \, | \, \bm \Yobs) - L(\theta^{(t)} \, | \, \bm \Yobs) \geq 0$ using Jensen's inequality. See for example the original EM paper by @dempster1977maximum for a detailed proof.


<!-- References -->
# References
