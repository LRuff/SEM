---
output: 
  pdf_document:
    fig_caption: true
    keep_tex: true
    latex_engine: pdflatex
    number_sections: true
header-includes:
- \usepackage[english]{babel}
- \usepackage{setspace}
- \hyphenation{}
- \pagenumbering{arabic}
- \newcommand{\Yobs}{Y_{\text{obs}}}
- \newcommand{\Ymis}{Y_{\text{mis}}}
- \newcommand{\E}{\mathbb{E}}
- \newcommand{\argmax}{\text{argmax}}
fontsize: 10pt
geometry: a4paper, left=30mm, right=30mm, top=20mm, bottom=20mm, includefoot
bibliography: references.bib
documentclass: article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<!-- Titlepage -->
\thispagestyle{empty}

\begin{raggedright}
  Freie Universit√§t Berlin\\
  Chair of Statistics\\
  Location: Berlin\\
  Winter Term 2016/2017\\
  Lecture: Missing Data Analysis\\
  Examiner: Dr. Florian Meinfelder
\end{raggedright}

\vspace*{\fill}
  \begin{center}
    \textbf{\Large{The Supplemented EM algorithm}}%
  \end{center}
\vfill

\begin{raggedleft}
  Carlo Michaelis (5043128), \href{mailto:carlo.michaelis@gmail.com}{carlo.michaelis@gmail.com}\\
  Lukas Ruff (5029054), \href{mailto:contact@lukasruff.com}{contact@lukasruff.com}\\
  \ \\
  Master Statistics\\
  \today\\
\end{raggedleft}

\newpage


<!-- Table of Contents -->
\tableofcontents
\newpage


<!-- List of Figures and Tables -->
\listoffigures
\listoftables
\newpage


<!-- Document Body -->
# Introduction

There are several methods to deal with missing data. Beside multiple imputation it is possible to estimate parameters by using the likelihood function. Unfortunatly in many cases to solve the likelihood of the observed data is not easy. Therefore the so called expectation maximization (EM) algorithm can be used. It was devloped by @dempster1977maximum to obtain maximum likelihood estimations when the underlying distribution depends on unobserved variables.

While the algortihm itself - finding parameters $\hat\theta$ - is feasible, the estimtaion of the variance of the estimators (e.g. to obtain standard errors) is somehow difficult. Different approaches were suggested to face this problem:

 * Resampling: Bootstrap/Jackknife
 * Multiple imputation
 * Supplemented EM (SEM)
 * ...

pros/cons of bootstrap and multiple imputation

@enders2010missing, @meng1991sem

In this term paper the goal is to describe the EM and SEM algorithm. The theory is done in general and afterwards applied to a bivariate normal problem. Furthermore we computed an example in `R` regarding the bivariate problem, which should give an intuition for the EM and the SEM algorithm.

# The EM Algorithm

The EM algorithm consists of two steps, namely the *expectation step* and the *maximization step*. Both steps are applied alternately until some convergence criterion is reached. Depending on given data structure, especially the missing data, we can find a *rate of convergence* $DM$ for the parameters. Both aspects of the EM algorithm are explained in this section.

## Objective and algorithm

The model for the complete data is given by

$$
f(Y|\theta),\quad \theta = (\theta_1, ..., \theta_d)
$$

and $Y$ can be separated into a observed $\Yobs$ and a missed part $\Ymis$, such that $Y = (\Yobs, \Ymis)$. We define $\theta^*$ as the maximum likelihood estima (MLE) of the parameter $\theta$. The estimation is based on the observed data $\Yobs$ and therefore maximizes $f(\Yobs|\theta)$. The algorithm starts with an initial parameter value $\theta^{(0)}$, which becomes $\theta^{(t)}$ at the `t`'hs EM iteration, which converges to $\theta^*$.

<!-- explain objective of em, how to obtain Q -->

**Expectation step**. The idea of the so called expectation step (*E step*) is to obtain the expectation value of the log-likelihood function related to the missing data, given the observed data and the parameter at iteration `t`, which is given by

$$
Q(\theta|\theta^{(t)}) = \E_{\Ymis|\Yobs,\theta^{(t)}}[L(\theta|Y)]  = \E_{\Ymis|\Yobs,\theta^{(t)}}[\ln f(Y|\theta)],
$$

while the objective can be calculated by

$$
\E_{\Ymis|Yobs,\theta^{(t)}}[L(\theta|Y)] = \int L(\theta|Y) f(\Ymis|\Yobs,\theta=\theta^{(t)})\,d\Ymis.
$$

**Minimization step**. In the Minimization step (*M step*) the objective of the E step will be maximized, which means that

$$
Q(\theta^{(t+1)}|\theta^{(t)}) \ge Q(\theta|\theta^{(t)}), \quad \forall \theta.
$$

More clearly the parameter $\theta^{(t+1)}$ is obtained by 

$$
\theta^{(t+1)} = \underset{\theta}{\argmax} \,Q(\theta|\theta^{(t)}).
$$

It means that we search for $\theta^{(t+1)}$ which maximizes the objective, given $\theta^{(t)}$ and we can now search for $Q(\theta|\theta^{(t+1)})$ using the E step, which starts the next iteration.

<!-- In box: Application to regression model, normal case -->

## Rate of convergence

The EM algorithm updates the parameter in every iteration of E step and M step from $\theta^{(t)}$ to $\theta^{(t+1)}$. To describe this behavior we can define a mapping

$$
M: \Theta \to \Theta,\; \theta \mapsto M(\theta)\quad \text{such that}\quad \theta^{(t+1)} = M(\theta^{(t)}),
$$

where $\Theta$ is the parameter space. While $\theta^{(t)}$ converges to $\theta^*$, we can state

$$
\theta^* = M(\theta^*),
$$

where $\theta^*$ is a fixed point of $M$.

To get a rate of convergence, we need a derivation of $M$, which we call $DM$. Therefore we do a taylor approximation of $\theta^{(t+1)}$ in the neighborhood of $\theta^*$ and stop after the linear term, which results in

\begin{align*}
\theta^{(t+1)} &= M(\theta^*) + M'(\theta^*) (\theta^{(t+1)} - \theta^*) + \frac{1}{2} M''(\theta^*)(\theta^{(t+1)} - \theta^*)^2 + ...\\
&\approx \theta^* + M'(\theta^*) (\theta^{(t+1)} - \theta^*).
\end{align*}

Rearraning the equation finally gives

$$
M'(\theta^*) = DM \approx \frac{\theta^{(t+1)} - \theta^*}{\theta^{(t)} - \theta^*},\quad \text{where}\quad DM = \left( \frac{\partial M_j(\theta)}{\partial \theta} \right) \big|_{\theta = \theta^*}.
$$

We call $DM$ the *rate matrix*.

<!-- describe mapping as combination of e-step, m-step -->
<!-- attributes of m -> fixed points -->
<!-- taylor -->
<!-- link later used -->
<!-- In box: Application to regression model, normal case -->

# The SEM algorithm

Introduction: Goal: variance of parameter estimation

## Large sample variance

<!-- combine 2.3/2.4 -->

derivation of theoretical variance V

explain why large sample

Explain relation to fisher information matrix (regularity / large sample)

proof 2.4

<!-- proof 2.4.5 optional -->

transition: Evaluation of V (1) Ioc (2) DM (3) V

## Ioc for exponential family

exponential family

regularity of expfam (link to fisher information, see above)

<!-- Optional: What to do if not regular? -->

In box: Application to regression model, normal case

## DM

r_ij component

component wise derivation of matrix

(1) EM to obtain theta (2) SEM to obtain r_ij

<!-- OPtional: Alternatives for approximation of DM -->



# Application

stopping criterion depending on symmetry of matrix (if matrix is very symmetric, stop em)

\newpage


<!-- References -->
# References
