---
output: 
  pdf_document:
    fig_caption: true
    keep_tex: true
    latex_engine: pdflatex
    number_sections: true
header-includes:
- \usepackage[english]{babel}
- \usepackage{setspace}
- \usepackage{bm}
- \hyphenation{}
- \pagenumbering{arabic}
- \numberwithin{equation}{section}
- \newcommand{\Yobs}{Y_{\text{obs}}}
- \newcommand{\yobs}{y_{\text{obs}}}
- \newcommand{\Ymis}{Y_{\text{mis}}}
- \newcommand{\ymis}{y_{\text{mis}}}
- \newcommand{\E}{\mathbb{E}}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\argmax}{\text{argmax}}
- \newcommand{\I}{\mathcal{I}}
fontsize: 10pt
geometry: a4paper, left=30mm, right=30mm, top=20mm, bottom=20mm, includefoot
bibliography: references.bib
documentclass: article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, echo=FALSE, warning=FALSE, message=FALSE}
library(MASS)
library(mixtools)
library(plyr)
library(xtable)
```

<!-- Titlepage -->
\thispagestyle{empty}

\begin{raggedright}
  Freie Universit√§t Berlin\\
  Chair of Statistics\\
  Location: Berlin\\
  Winter Term 2016/2017\\
  Lecture: Missing Data Analysis\\
  Examiner: Dr. Florian Meinfelder
\end{raggedright}

\vspace*{\fill}
  \begin{center}
    \textbf{\Large{The Supplemented EM algorithm}}%
  \end{center}
\vfill

\begin{raggedleft}
  Carlo Michaelis (5043128), \href{mailto:carlo.michaelis@gmail.com}{carlo.michaelis@gmail.com}\\
  Lukas Ruff (5029054), \href{mailto:contact@lukasruff.com}{contact@lukasruff.com}\\
  \ \\
  Master Statistics\\
  \today\\
\end{raggedleft}

\newpage


<!-- Table of Contents -->
\tableofcontents
\newpage


<!-- List of Figures and Tables -->
\listoffigures
\listoftables
\newpage


<!-- Document Body -->
# Introduction

There are several methods to deal with missing data. Besides multiple imputation, it is possible to estimate parameters using the likelihood function. Unfortunatly in many cases, solving the likelihood of the observed data is not an easy task. Therefore the so called expectation maximization (EM) algorithm can be used. It was developed by @dempster1977maximum to obtain maximum likelihood estimations when the underlying distribution depends on unobserved variables.

While the algortihm itself - finding parameters $\bm{\hat\theta}$ - is feasible, the estimtaion of the variance of the estimators (e.g. to obtain standard errors) is somehow difficult. Different approaches were suggested to face this problem:

 * Algebraic analysis [@meilijson1989fast]
 * Monte Carlo evaluation using multiple imputation [@rubin2004multiple]
 * Numerical methods [@carlin1987seasonal]
 * Resampling: Bootstrap/Jackknife
 * Supplemented EM (SEM)

While methods like resampling or algebraic analysis mostly depend on i.i.d. data, SEM can be done without this assumption. Furthermore numerical methods try to calculate the variance-covariance matrix directly, while SEM just computes the increase of variance, which is computational more stable. Lastly Monte Carlo (MC) evaluation needs more extra code and the result depends on the number of imputations. In the 1990s, when the SEM algorithm was developped by @meng1991sem, the computational power was far less than today, where it was not possible to do as many imputations as are possible today.

In this term paper the goal is to describe the EM and SEM algorithm. The theory is done in general and afterwards applied to a bivariate normal problem. Furthermore we computed an example in `R` regarding the bivariate problem, which should give an intuition for the EM and the SEM algorithm. Finally a simulation study was done, where analytical results are compared with results from SEM and MC.

# A Recap of the EM Algorithm

The Expectation Maximization (EM) algorithm is an iterative procedure to maximize the likelihood of a model with missing or latent variables. It consists of two steps, namely the *expectation step* and the *maximization step*. Both steps are applied alternately until some convergence criterion is reached. Depending on the given data structure, especially the missing data, we can find a *rate of convergence* $DM$ for the parameters. Both aspects of the EM algorithm are explained in this section.

## Objective and algorithm

Let the model of the complete data $\bm Y$, which has an observed part $\bm \Yobs$ and missing values $\bm \Ymis$, that is $\bm Y = (\bm \Yobs, \bm \Ymis)$, be described by the complete-data density $f(\bm Y \, | \, \bm \theta)$, $\bm \theta = (\theta_1, ..., \theta_d)$, and denote the complete-data log-likelihood by

$$
L(\bm \theta \, | \, \bm Y) = \log f(\bm Y \, | \, \bm \theta),
$$

that is we assume the missing-data mechanism to be ignorable. The EM algorithm finds the maximum likelihood estimate (MLE) of the parameter $\bm \theta$ of the *observed* likelihood $f(\bm \Yobs \, | \, \bm \theta)$, which we define by $\bm{\hat \theta}$. The key feature of the EM algorithm is, that this maximization of the observed (log-)likelihood is not carried out directly, but indirectly and iteratively by maximizing the conditional expectation over $f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta^{(t)})$ of the complete-data log-likelihood $L(\bm \theta \, | \, \bm Y)$, where $\bm \theta^{(t)}$ is the parameter value at the $t$-th iteration. The algorithm starts with some initial parameter value $\bm \theta^{(0)}$.

**Expectation step**. The expectation step (*E step*) computes the conditional expectation of the complete-data log-likelihood over $f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta^{(t)})$ using the parameter of iteration $t$, that is

$$
Q(\bm \theta \, | \, \bm \theta^{(t)}) = \E[L(\bm \theta \, | \, \bm Y) \, | \, \bm \Yobs, \bm \theta^{(t)}]  = \int L(\bm \theta \, | \, \bm Y) f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta^{(t)}) \, d\bm \Ymis.
$$

Therefore, the E step estimates the missing data parts of the complete-data log-likelihood by the expectational value using the observed data and the current best parameter estimates $\bm \theta^{(t)}$ of the model.


**Maximization step**. In the maximization step (*M step*) the objective of the E step is maximized in $\bm \theta$ in order to obtain an update of the model parameters, which means solving

$$
\bm \theta^{(t+1)} = \underset{\bm \theta}{\argmax} \,Q(\bm \theta \, | \, \bm \theta^{(t)}), \qquad \text{ i.e. } \, Q(\bm \theta^{(t+1)} \, | \, \bm \theta^{(t)}) \geq Q(\bm \theta \, | \, \bm \theta^{(t)}), \quad \forall \bm \theta.
$$

The updated parameters $\bm \theta^{(t+1)}$ are always better in the sense, that they have a greater or equal observed-data (log-)likelihood than the previous parameters $\bm \theta^{(t+1)}$, that is $L(\bm \theta^{(t+1)} \, | \, \bm \Yobs) \geq L(\bm \theta^{(t)} \, | \, \bm \Yobs)$.[^1]
After obtaining the parameter update $\bm \theta^{(t+1)}$, it can be used in the following E and M step which conduct the next EM iteration.  

For the large class of exponential families, the EM algorithm is usually easy to implement: The M step is identical to the computation of ML estimates from the complete-data log-likelihood $L(\bm \theta \, | \, \bm Y)$; the E step for most problems only involves calculating the conditional expectation of the sufficient statistics, which are easy to compute from the moments of the conditional distribution.

## Rate of convergence

The EM algorithm updates the parameter in every iteration of E step and M step from $\bm \theta^{(t)}$ to $\bm \theta^{(t+1)}$. To describe this behavior we can define a mapping

$$
M: \Theta \to \Theta,\; \bm \theta \mapsto M(\bm \theta)\quad \text{such that}\quad M(\bm \theta^{(t)}) = \bm \theta^{(t+1)}, \, t = 0, 1, \ldots \;
$$

where $\Theta$ is the parameter space. If $\left(\bm \theta^{(t)}\right)$ converges to $\bm{\hat \theta}$ and $M$ is continuous, we have

$$
M(\bm{\hat \theta}) = \bm{\hat \theta},
$$

that is $\bm{\hat \theta}$ is a fixed point of $M$. Thus, in the neighborhood of $\bm{\hat \theta}$, we get 

\begin{align*}
M(\bm \theta^{(t)}) &\approx M(\bm{\hat \theta}) + DM (\bm \theta^{(t)} - \bm{\hat \theta})\\
\iff \qquad \bm \theta^{(t+1)} &\approx \bm{\hat \theta} + DM (\bm \theta^{(t)} - \bm{\hat \theta})\\
\iff \, \bm \theta^{(t+1)} - \bm{\hat \theta} &\approx DM (\bm \theta^{(t)} - \bm{\hat \theta})
\end{align*}

doing a taylor approximation of $M(\bm \theta^{(t)})$ at $\bm{\hat \theta}$ and stopping after the linear term, where

\begin{equation}
\label{eq:rate}
DM = \left.\left( \frac{\partial M_j(\bm \theta)}{\partial \theta_i} \right) \right|_{\bm \theta = \bm{\hat \theta}}
\end{equation}

is the $d \times d$ Jacobian matrix of $M(\bm \theta) = (M_1(\bm \theta), \ldots, M_d(\bm \theta))$ evaluated at $\bm{\hat \theta}$. Hence, in the neighborhood of $\bm{\hat \theta}$, the EM algorithm converges approximately linear with rate matrix $DM$.


# The SEM algorithm

In comparison to multiple imputation methods, asymptotic variance-covariance matrices of the parameter estimaton $\bm{\hat\theta}$ (e.g. standard errors) are not given automatically when using the EM algorithm. Instead, additional steps are required in order to compute standard errors of the estimates. One approach introduced by @meng1991sem is the so-called *supplemented EM* or *SEM* algorithm. This procedure obtains numerically stable asymptotic variance-covariance matrices only from computing the complete-data asymptotic variance-covariance matrix, using code for the E and M steps of EM, and standard matrix operations. The key idea of the SEM algorithm is effectively to numerically differentiate the implicit mapping $\bm \theta \mapsto M(\bm \theta)$ defined by the EM algorithm, i.e. defining a method to numerically approximate the Jacobian matrix $DM$.


## Asymptotic variance-covariance matrix of $(\bm \theta - \bm{\hat \theta})$ based on $\bm \Yobs$
\label{sec:asy-cov}

The desired asymptotic variance-covariance matrix of $(\bm \theta - \bm{\hat \theta})$ based on $\bm \Yobs$ is given by the inverse of the observed information matrix[^3], i.e.

$$
V = \I_{o}^{-1}(\bm{\hat \theta} \, | \, \bm \Yobs),
$$

where the observed information matrix $\I_{o}(\bm \theta \, | \, \bm \Yobs)$ is the negative Hessian of the log-likelihood of the observed data,

\begin{equation}
\label{eq:Iobs}
\I_{o}(\bm \theta \, | \, \bm \Yobs) = -\frac{\partial^2 \log f(\bm \Yobs \, | \, \bm \theta)}{\partial\bm \theta \cdot \partial \bm \theta}.
\end{equation}

The problem is that it can often be very difficult to evaluate (\ref{eq:Iobs}) directly. Therefore, another representation of $V$ is preferred. Precisely, we will derive the following decomposition of $V$:

\begin{equation}
\label{eq:V}
V = V_c + \Delta V \qquad \text{ with } \, V_c = \I_{oc}^{-1},
\end{equation}

where $V_c$ is the asymptotic complete-data variance-covariance matrix given by the inverse of the conditional expectation over $f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta)$ of the complete-data information matrix evaluated at $\bm \theta = \bm{\hat \theta}$, that is

\begin{equation}
\label{eq:Ioc}
\I_{oc} = \E[\I_{c}(\bm \theta \, | \, \bm Y) \, | \, \bm \Yobs, \bm \theta ] \bigg\rvert_{\bm \theta = \bm{\hat \theta}} \; , \qquad \I_{c}(\bm \theta \, | \, \bm Y) = -\frac{\partial^2 \log f(\bm Y \, | \, \bm \theta)}{\partial \bm \theta \cdot \partial \bm \theta},
\end{equation}

and $\Delta V$, the increase due to missing data, is given by

\begin{equation}
\label{eq:DeltaV}
\Delta V =  V_c \, DM \, (I_d - DM)^{-1},
\end{equation}

where $DM$ is the rate matix from (\ref{eq:rate}) and $I_d$ is the $d$-dimensional identity matrix. We will now show, why the decomposition in (\ref{eq:V}) holds.

## Proof that $V = V_c + \Delta V$

From Bayes' rule on densities, we have the factorization

$$
f(\bm Y \, | \, \bm \theta) = f(\bm \Yobs \, | \, \bm \theta) \, f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta),
$$

and therefore it follows for the log-likelihood of $\bm \theta$ given $\bm \Yobs$

\begin{align*}
L(\bm \theta \, | \, \bm \Yobs) &= \log f(\bm \Yobs \, | \, \bm \theta)\\
&= \log f(\bm Y \, | \, \bm \theta) - \log f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta) = L(\bm \theta \, | \, \bm Y) - \log f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta).
\end{align*}

Taking second derivatives and the expectation over the conditional distribution $f(\bm \Ymis \, | \, \bm \Yobs , \bm \theta)$, we further get that

\begin{align*}
&&\E\left[\frac{\partial^2 L(\bm \theta \, | \, \bm \Yobs)}{\partial \bm \theta \cdot \partial \bm \theta} \, \Big| \, \bm \Yobs, \bm \theta \right] &= \E\left[\frac{\partial^2 L(\bm \theta \, | \, \bm Y)}{\partial \bm \theta \cdot \partial \bm \theta} \, \Big| \, \bm \Yobs, \bm \theta \right] - \E\left[\frac{\partial^2 \log f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta)}{\partial \bm \theta \cdot \partial \bm \theta} \, \Big| \, \bm \Yobs, \bm \theta \right]\\
\iff &&-\frac{\partial^2 L(\bm \theta \, | \, \bm \Yobs)}{\partial \bm \theta \cdot \partial \bm \theta} &= \E\left[-\frac{\partial^2 L(\bm \theta \, | \, \bm Y)}{\partial \bm \theta \cdot \partial \bm \theta} \, \Big| \, \bm \Yobs, \bm \theta \right] - \E\left[-\frac{\partial^2 \log f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta)}{\partial \bm \theta \cdot \partial \bm \theta} \, \Big| \, \bm \Yobs, \bm \theta \right] \\
\iff &&\I_o(\bm \theta \, | \, \bm \Yobs) &= \E\left[\I_c(\bm \theta \, | \, \bm Y) \, | \, \bm \Yobs, \bm \theta \right] - \E\left[-\frac{\partial^2 \log f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta)}{\partial\bm \theta \cdot \partial\bm \theta} \, \Big| \, \bm \Yobs, \bm \theta \right].
\end{align*}

Evaluating this equation at $\bm \theta = \bm{\hat \theta}$ gives

\begin{equation}
\label{eq:informationprinciple}
\I_o(\bm{\hat \theta} \, | \, \bm \Yobs) = \I_{oc} - \I_{om},
\end{equation}

where 

$$
\I_{om} = \left. \E\left[-\frac{\partial^2 \log f(\bm \Ymis \, | \, \bm \Yobs, \bm \theta)}{\partial\bm \theta \cdot \partial\bm \theta} \, \Big| \, \bm \Yobs, \bm \theta \right] \right\rvert_{\bm \theta = \bm{\hat \theta}}
$$

can be viewed as the missing information. Equation (\ref{eq:informationprinciple}) is also called *missing information principle* with the nice interpretation

$$
\textit{observed information} = \textit{complete information } - \textit{missing information}.
$$

Besides, @dempster1977maximum showed that when maximizing $Q(\bm \theta \, | \, \bm \theta^{(t)})$ in the M step of the EM algorithm, i.e. when setting the first derivative of $Q(\bm \theta \, | \, \bm \theta^{(t)})$ w.r.t. $\bm \theta$ equal to zero, it holds that

$$
DM = \I_{om} \I_{oc}^{-1}.
$$

This can be used with (\ref{eq:informationprinciple}) to obtain

$$
\I_o(\bm{\hat \theta} \, | \, \bm \Yobs) = \I_{oc} - \I_{om} = (I_d - \I_{om}\I_{oc}^{-1})\I_{oc} = (I_d - DM)\I_{oc}.
$$

Finally, by inverting both sides of this equation, we get the decomposition of the asymptotic variance-covariance matrix based on $\bm \Yobs$:

\begin{align*}
V = \I_o^{-1}(\bm{\hat \theta} \, | \, \bm \Yobs) &= \I_{oc}^{-1}(I_d - DM)^{-1}\\
&= \I_{oc}^{-1}(I_d - DM + DM)(I_d - DM)^{-1}\\
&= \I_{oc}^{-1} + \I_{oc}^{-1} DM (I_d - DM)^{-1}\\
&= \I_{oc}^{-1} + \Delta V = V_c + \Delta V.
\end{align*}

This representation of $V$ is the ansatz of the SEM algorithm, which consists of three parts: (1) the evaluation of $V_c = \I_{oc}^{-1}$, (2) the numerical approximation of $DM$, and (3) using (\ref{eq:V}) with (\ref{eq:DeltaV}) to estimate $V$. We look into the evaluation of $V_c = \I_{oc}^{-1}$ in the next subsection.

<!-- Optional TODO: proof 2.4.5 -->


## Evaluation of $V_c$ for exponential families

We consider the evaluation of $V_c = \I_{oc}^{-1}$ for the case that the complete-data density $f(\bm Y \, | \, \bm \theta)$ is from an *exponential family*, since this is most common in practical applications. It means that the complete-data density has the following representation

$$
f(\bm Y \, | \, \bm \theta) = h(\bm Y) \exp\{\eta(\bm \theta)^T T(\bm Y) - A(\bm \theta)\} = h(\bm Y) \exp\left\{\sum_{i=1}^s\eta_i(\bm \theta) T_i(\bm Y) - A(\bm \theta) \right\},
$$

where the *natural parameter* $\eta(\bm \theta)$ and the *sufficient statistics* $T(\bm Y)$ are vector functions in $\R^s, s \geq d,$ and the *base measure* $h(\bm Y)$ as well as the *log-normalizer* $A(\bm \theta)$ are scalar functions.

The sufficient statistics $T(\bm Y)$ is crucial for the implementation and application of the EM and SEM algorithm for exponential families. This is because we have

$$
L(\bm \theta \, | \, \bm Y) = \log f(\bm Y \, | \, \bm \theta) = \log h(\bm Y) + \eta(\bm \theta)^T T(\bm Y) - A(\bm \theta)
$$

which means that the complete-data log-likelihood is linear in the sufficient statistics $T(\bm Y)$, i.e. $L(\bm \theta \, | \, \bm Y) = L(\bm \theta \, | \, T(\bm Y))$. Therefore, in order to compute the E step for exponential families, we practically just have to evaluate the conditional expectation of the sufficient statistics $\E[T(\bm Y) \, | \, \bm \Yobs, \bm \theta^{(t)}]$. Furthermore, linearity in the log-likelihood implies that the complete-data information is also linear in $T(\bm Y)$, that is $\I_{c}(\bm \theta \, | \, \bm Y) = \I_{c}(\bm \theta \, | \, T(\bm Y))$. Therefore, from the definition of $\I_{oc}$ in (\ref{eq:Ioc}), we get that

\begin{align*}
\I_{oc} &= \E[\I_{c}(\bm \theta \, | \, \bm Y) \, | \, \bm \Yobs, \bm \theta ] \bigg\rvert_{\bm \theta = \bm{\hat \theta}}\\
&= \I_{c}(\bm{\hat \theta} \, | \, T^*(\bm \Yobs)),
\end{align*}

by linearity in $T(\bm Y)$ with $T^*(\bm \Yobs) = \E[T(\bm Y) \, | \, \bm \Yobs, \bm{ \hat \theta}]$, which is obtained in the very last E step of the EM procedure. Hence, we can obtain $V_c = \I_{oc}^{-1}$ simply by plugging in the conditional expectation of the sufficient statistics $T^*(\bm \Yobs)$ computed in the last E step into the inverse of $\I_{c}(\bm \theta \, | \, T(\bm Y))$ evaluated at $\bm \theta = \bm{\hat \theta}$.

When the exponential family is regular (i.e. when $s = d$ and the Jacobian of $\eta(\bm \theta)$ has full rank) the evaluation of $V_c = \I_{oc}^{-1}$ is even simpler. In this case, the complete-data information matrix $\I_{oc}$ is just the usual Fisher information matrix evaluated in $\bm{\hat \theta}$, i.e.

$$
\I_{oc} = \I(\bm{\hat \theta}), \qquad \I(\bm \theta) = \E[\I_c(\bm \theta \, | \, \bm Y) \, | \, \bm \theta].
$$

Inverting the Fisher information then gives $V_c$.

For the case when the complete-data density $f(\bm Y \, | \, \theta)$ is not from an exponential family, the complete-data log-likelihood $L(\bm \theta \, | \, \bm Y)$ is no longer linear in the sufficient statistics $T(\bm Y)$. This can make the evaluation of the E step difficult, because then $Q(\bm \theta \, | \, \bm \theta^{(t)})$ is generally not of closed form. One approach to tackle this problem is to linearize $L(\bm \theta \, | \, \bm Y)$ in the sufficient statistics using a Taylor expansion. See @dempster1977maximum section 3.2 for further details.


## Numerical Approximation of DM

Let $r_{ij}$ be the $(i, j)$-th element of the rate matrix $DM$, that is

$$
DM = \left.\left( \frac{\partial M_j(\bm \theta)}{\partial \theta_i} \right) \right|_{\bm \theta = \bm{\hat \theta}} = \left(r_{ij}\right), \quad i,j = 1,\ldots,d.
$$

Define $\bm \theta^{(t)}(i)$ to be the ML estimate $\bm{\hat \theta}$ with varied $i$-th component:

\begin{equation}
\label{eq:theta_t_i}
\bm \theta^{(t)}(i) = (\hat \theta_1, \ldots, \hat \theta_{i-1}, \theta^{(t)}_i, \hat \theta_{i+1}, \ldots, \hat \theta_d).
\end{equation}

That is, only the $i$-th component is active and all others are fixed at the ML estimate. Then, from the definition of $r_{ij}$ and writing the componentwise derivatives as limits of the difference quotients, we have

\begin{align*}
r_{ij} &= \frac{\partial M_j (\bm{\hat \theta})}{\partial \theta_i}\\
&= \lim_{\theta_i \to \hat \theta_i} \frac{M_j(\hat \theta_1, \ldots, \hat \theta_{i-1}, \theta_i, \hat \theta_{i+1}, \ldots, \hat \theta_d) - M_j(\hat \theta)}{\theta_i - \hat \theta_i}\\
&= \lim_{t \to \infty} \underbrace{\frac{M_j(\bm \theta^{(t)}(i)) - \hat \theta_j}
{\theta_i^{(t)} - \hat \theta_i}}_{=: r_{ij}^{(t)}} = \lim_{t \to \infty} r_{ij}^{(t)},
\end{align*}

if the EM algorithm converges, i.e. when $\lim_{t \to \infty} \theta_i^{(t)} = \hat \theta_i$. Since $M(\bm \theta)$ is defined implicitly by the EM algorithm, the rate sequences $(r_{ij}^{(t)})_t$ can be computed using the code of the E and M steps of the EM algorithm. Hence, after obtaining the ML estimate $\bm{\hat \theta}$ from some estimation procedure (e.g. also the EM algorithm), we can numerically approximate $DM$ only from using code of the EM algorithm. We summarize the complete SEM algorithm in the next subsection. 

<!-- Optional: Alternatives for approximation of DM -->

<!-- ## Notes -->
<!-- It is important to emphasize that the variance-covariance matrix obtained from SEM is based on the second derivatives of the observed-data log-likelihood and only is inferentially valid asymptotically, i.e. for large samples. -->
<!-- The more normal the likelihood, the better the approximation. Therefore, transformations are useful in order to improve the results. (e.g. log(variance), etc.) -->

## The algorithm

From the steps outlined above, we can summarize the overall SEM algorithm as follows:

\textbf{Data:} $\bm \yobs$

\textbf{Step 1:} Obtain the ML estimate $\bm{\hat \theta}$ by EM (or any other procedure, e.g. also from closed-form solutions in special cases).

\textbf{Step 2:} Evaluate $V_c = \I_{oc}^{-1}$ as outlined in section 3.3 (i.e. for regular exponential families compute the inverse of the Fisher information matrix evaluated at $\bm{\hat \theta}$).

\textbf{Step 3:} Computation of DM  
Starting with a value $\bm{\theta}^{(0)} \not =\bm{\hat \theta}$ run the following sequence of SEM iterations:

*Input*: $\bm{\hat \theta}$ and $\bm \theta^{(t)}$  
*Step 3.1:* Run the usual E and M steps with $\bm \theta^{(t)}$ to obtain $\bm \theta^{(t+1)}$.  
Repeat steps 3.2 and 3.3 for $i = 1,\ldots,d.$  
*Step 3.2:* Compute $\bm \theta^{(t)}(i)$ from (\ref{eq:theta_t_i}) and use it to run one iteration of EM to obtain $\bm{\tilde\theta}^{(t+1)}(i)$.  
*Step 3.3:* Obtain one iteration of the rate sequences by
\begin{equation}
\label{eq:rateseq}
r_{ij}^{(t)} = \frac{\tilde\theta_j^{(t+1)}(i) - \hat\theta_j}{\theta_i^{(t)} - \hat\theta_i}, \quad \text{for } j = 1, \ldots, d.
\end{equation}
*Output:* $\bm \theta^{(t+1)}$ and $\{r_{ij}^{(t)}, \; i,j = 1,\ldots, d \}$.

Obtain the limit $r_{ij}$ when the sequence $(r_{ij}^{(t)})$ is stable for some $t^*$, which can be different for each pair $(i,j)$. This means that steps 3.2 and 3.3 are only repeated for the pairs that have not yet converged. The numerical approximation of $DM = (r_{ij})$ is done, when all rate sequences have converged.

\textbf{Step 4:} Evaluate $V = V_c + \Delta V$ with the results from step 2 and step 3 with equation \eqref{eq:DeltaV}.


# Application

In this section, we will apply the SEM algorithm to a bivariate normal example. To verify our implementation, we will first reproduce the results of @meng1991sem in their bivariate normal example. Afterwards, we will conduct a simulation study to evaluate the quality of the SEM algorithm by comparing the results to the direct estimation of the large-sample observed-data covariance matrix, which is possible to compute in the bivariate normal example, and by performing the simulation for different missing-data mechanisms. Let's first introduce the bivariate normal model.

## A bivariate normal model

We consider an i.i.d. sample of size $n$ from a bivariate normal distribution:

$$
\left(\begin{array}{c} Y_{1,1}\\ Y_{1,2}\\ \end{array} \right), \ldots, \left(\begin{array}{c} Y_{n,1}\\ Y_{n,2}\\ \end{array} \right) \; \overset{\text{i.i.d}}{\sim} \; N\left( \bm \mu = \left(\begin{array}{c} \mu_1\\ \mu_2\\ \end{array} \right), \bm \Sigma = \left(\begin{array}{cc} \sigma_{11} & \sigma_{12}\\ \sigma_{21} & \sigma_{22}\\ \end{array} \right) \right),
$$

where $\bm Y_1 = (Y_{1,1}, \ldots, Y_{n,1})$ is fully observed but from $\bm Y_2 = (Y_{1,2}, \ldots, Y_{n,2})$ we only observe $m < n$ variables and consequently $n-m$ variables are missing. This means that $\bm Y = (\bm Y_1, \bm Y_2) = (\bm \Yobs, \bm \Ymis)$ with

\begin{align*}
\bm \Yobs &= (Y_{1,1}, \ldots, Y_{n,1}, Y_{1,2}, \ldots, Y_{m,2}) \quad \text{ and}\\
\bm \Ymis &= (Y_{m+1,2}, \ldots, Y_{n,2}).
\end{align*}

Since $\sigma_{12} = \sigma_{21}$, the bivariate normal is fully characterized by $\bm \theta = (\mu_1, \mu_2, \sigma_{11}, \sigma_{22}, \sigma_{12})$ and has density $f_1$ is given by

$$
f_1(y_1, y_2 \, | \, \bm \theta) = (2\pi)^{-1} |\bm \Sigma|^{-\frac{1}{2}} \exp\left(- \frac{1}{2} (\bm y - \bm \mu)^T \bm \Sigma^{-1} (\bm y - \bm \mu) \right)
$$
with

$$
|\bm \Sigma| = \sigma_{11} \sigma_{22} - \sigma_{12}^2 \; \text{ and } \; \bm \Sigma^{-1} = |\bm \Sigma|^{-1} \left(\begin{array}{cc} \sigma_{22} & -\sigma_{12}\\ - \sigma_{12} & \sigma_{11}\\ \end{array} \right).
$$


### The observed-data log-Likelihood

Recall that in presence of missing-data, the theoretically correct ML estimate would be the parameter maximizing the (log-)likelihood of the *observed* data. For the sample $\bm \Yobs$, we would have the following observed-data log-likelihood:

\begin{align*}
L(\bm \theta \, | \, \bm \yobs) = &\log f(\bm \yobs \, | \, \bm \theta)\\
= &\sum_{i=1}^n \log f_{\Yobs} (\yobs \, | \, \bm \theta)\\
= &\sum_{i=1}^m \log f_{1} (y_{i1}, y_{i2}  \, | \, \bm \theta) + \sum_{i=m+1}^n \log f_{1} (y_{i1}  \, | \, \mu_1, \sigma_{11})\\
= &-m \log 2 \pi - \frac{m}{2} \log |\bm \Sigma| - \frac{1}{2} \sum_{i=1}^m (\bm y_i - \bm \mu)^T \bm \Sigma^{-1} (\bm y_i - \bm \mu)\\
&-\frac{n-m}{2} \log 2 \pi - \frac{n-m}{2} \log \sigma_{11} - \frac{1}{2 \sigma_{11}} \sum_{i=m+1}^n (y_{i1} - \mu_1)^2
\end{align*}

because for samples $m+1$ to $n$ we only observe $Y_1$. Since for the bivariate normal, we are able to state the asymptotic variance-covariance matrix based on $\bm \Yobs$ directly, this will serve as a benchmark for the SEM algorithm.

### The complete-data log-likelihood

As we have seen, the EM algorithm uses the conditional expectation of the complete-data log-likelihood $Q(\bm \theta \, | \, \bm \theta^{(t)}) = \E[L(\bm \theta \, | \, \bm Y) \, | \, \bm \Yobs, \bm \theta^{(t)}]$ to iteratively approximate the ML estimate of the (log-)likelihood of the observed data. The complete-data log-likelihood of the bivariate normal sample $\bm Y$ is given by

\begin{align*}
L(\bm \theta \, | \, \bm y) = &\log f(\bm y \, | \, \bm \theta)\\
= &\sum_{i=1}^n \log f_1(y_{i1}, y_{i2} \, | \, \bm \theta)\\
= &-n \log 2 \pi - \frac{n}{2} \log |\bm \Sigma| - \frac{1}{2} \sum_{i=1}^n (\bm y_i - \bm \mu)^T \bm \Sigma^{-1} (\bm y_i - \bm \mu)\\
= &-n \log 2 \pi - \frac{n}{2} \log |\bm \Sigma| - \frac{1}{2} |\bm \Sigma|^{-1} \Big \{ \sigma_{22} T_{11} + \sigma_{11} T_{22} - 2 \sigma_{12} T_{12}\\
&- 2 \Big( T_1 (\mu_1 \sigma_{22} - \mu_2 \sigma_{12}) + T_2 (\mu_2 \sigma_{11} - \mu_1 \sigma_{12}) \Big)\\
&+ n (\mu_1^2 \sigma_{22} + \mu_2^2 \sigma_{11} - 2 \mu_1 \mu_2 \sigma_{12}) \Big \}
\end{align*}

since the bivariate normal is an exponential family and therefore the log-likelihood is linear in the sufficient statistics

$$
T_1 = \sum_{i=1}^n y_{i1}, \quad T_2 = \sum_{i=1}^n y_{i2}, \quad T_{11} = \sum_{i=1}^n y_{i1}^2, \quad T_{22} = \sum_{i=1}^n y_{i2}^2, \quad T_{12} = \sum_{i=1}^n y_{i1} y_{i2}.
$$

### E step and M step

As we can see from the complete-data log-likelihood, in order to compute the conditional expectation of the complete-data log-likelihood $Q(\bm \theta \, | \, \bm \theta^{(t)}) = \E[L(\bm \theta \, | \, \bm Y) \, | \, \bm \Yobs = \bm \yobs, \bm \theta^{(t)}]$ for given data $\bm \Yobs = \bm \yobs$ and currently best parameter $\theta^{(t)}$, we just have to compute the conditional expectational values of the sufficient statistics (as we already concluded in section 3.3 for exponential families in general). Since the samples $(Y_{1,1}, Y_{1,2}), \ldots, (Y_{n,1}, Y_{n,2})$ are independent, we only have to evaluate

$$
\E[Y_{i2} \, | \, Y_{i1} = y_{i1}, \bm \theta^{(t)}] \; \text{ and } \; \E[Y_{i2}^2 \, | \, Y_{i1} = y_{i1}, \bm \theta^{(t)}]
$$

for the missing samples $i = m+1, \ldots, n$. Because for the bivariate normal, the conditional distribution is given by

$$
\left( Y_2 \, | \, Y_1 = y_1 \right) \sim N \left(\mu_2 + \frac{\sigma_{12}}{\sigma_{11}}(y_1 - \mu_1), \sigma_{22}(1-\rho^2) \right), \quad \rho = \frac{\sigma_{12}}{\sqrt{\sigma_{11} \sigma_{22}}},
$$

we get the following **E step**:

\begin{align*}
\E[Y_{i2} \, | \, Y_{i1} &= y_{i1}, \bm \theta^{(t)}] = \mu_2^{(t)} + \left(\frac{\sigma_{12}^{(t)}}{\sigma_{11}^{(t)}}\right)(y_{i1} - \mu_1^{(t)}) = \underbrace{\mu_2^{(t)} - \left(\frac{\sigma_{12}^{(t)}}{\sigma_{11}^{(t)}}\right) \mu_1^{(t)}}_{=: \beta_0^{(t)}} + \underbrace{\left(\frac{\sigma_{12}^{(t)}}{\sigma_{11}^{(t)}}\right)}_{=: \beta_1^{(t)}} y_{i1} =: y_{i2}^{(t)}\\
\E[Y_{i2}^2 \, | \, Y_{i1} &= y_{i1}, \bm \theta^{(t)}] = \sigma_{22}^{(t)} (1 - {\rho^{(t)}}^2) + \big(y_{i2}^{(t)}\big)^2
\end{align*}

for the missing values $i = m+1, \ldots, n$ in the $t$-th iteration which is familiar from linear regression ($\hat y = \hat \beta_0 + \hat \beta_1 x$ with $\hat \beta_0 = \bar y - \hat \beta_1 \bar x$). Using the conditional expectational values with the usual complete-data ML estimators

\begin{align*}
\hat \mu_k &= \frac{1}{n} \, T_k , \quad k = 1,2\\
\hat \sigma_{kl} &= \frac{1}{n} \, \left( T_{kl} - \frac{1}{n} T_k T_l \right), \quad k,l = 1,2.
\end{align*}

then gives the parameter updates $\bm \theta^{(t+1)}$ of the **M step**. Note that since there are only missing values for $Y_2$, the parameters of $Y_1$ converge immediately after the first EM iteration with $\hat \mu_1 = \bm{\bar y_1}$ and $\hat \sigma_{11} = T_{11}/n - \bm{\bar y_1}^2$.

### No missing information in some components

In a bivariate case, where only one variable contains missing data and the other variable is complete, the calculation of $V$ changes. Therefore remind equation \eqref{eq:V} in section \ref{sec:asy-cov}. In this case the parameter $\bm\theta$ for the complete data converges in just one step, indipendent of the initial value. The denominator of equation \eqref{eq:rateseq} will directly be zero and therefore a rate cannot be obtained using SEM. Since we know that $M(\bm\theta)$ for the complete data component is constant, we can directly state that $r_{ij} = 0$ for that component and its calculation is not necessary. Therefore the matrix $DM$ is given by

$$
DM = \begin{pmatrix} 0 & A\\ 0 & DM^* \end{pmatrix},
$$

where $DM^*$ is a quadratic submatrix of size $d_2 \times d_2$ containing the parameter rates for the incomplete data and $A$ is a matrix of size $d_1 \times d_2$. The upper left area is a $d_1 \times d_1$ matrix, where $d_1$ is the dimension of the parameters from the complete data and $d_2$ the dimension of the missing data parameters, where holds that $d_1 + d_2 = d$. If we assume that $I_{oc}^{-1}$ is given by

$$
I_{oc}^{-1} = \begin{pmatrix} G_1 & G_2\\ G_2^T & G_3 \end{pmatrix},
$$

the variance $V$ is can be stated as

$$
V = I_{oc}^{-1} + \begin{pmatrix} 0 & 0\\ 0 & \Delta V^* \end{pmatrix} = \begin{pmatrix} G_1 & G_2\\ G_2^T & G_3 + \Delta V^* \end{pmatrix}.
$$

The matrix

$$
\Delta V^* = (G_3 - G_2^TG_1^{-1}G_2) DM^*(I_{d_2}-DM^*)^{-1}
$$

is proven in appendix of @meng1991sem, where matrix $A$ is identified with $A = -G_1^{-1} G_2 DM^*$.

### The complete-data Fisher information

Since the bivarate normal is also a *regular* exponential family, in order to evaluate the asymptotic complete-data variance-covariance matrix $V_c = \I_{oc}^{-1}$, we can use the standard Fisher information matrix $\I(\bm \theta)$ as mentioned in section 3.3 because for regular exponential families we have $\I_{oc} = \I(\bm \theta)$. The Fisher information for the bivariate normal can be derived from the general expression of the Fisher information for the multivariate normal distribution.[^2] We get

$$
\I_n(\bm \theta) = n \cdot \I(\bm \theta) = n \cdot |\bm \Sigma|^{-2} \left(\begin{array}{ccccc} 
\sigma_{22} |\bm \Sigma| & \cdots & \cdots & \cdots & \cdots \\
- \sigma_{12} |\bm \Sigma| & \sigma_{11} |\bm \Sigma| & \cdots & \cdots & \cdots \\
0 & 0 & \frac{1}{2} \sigma_{22}^2 & \cdots & \cdots \\
0 & 0 & \frac{1}{2} \sigma_{12}^2 & \frac{1}{2} \sigma_{11}^2 & \cdots \\
0 & 0 & -\sigma_{22} \sigma_{12} & -\sigma_{11} \sigma_{12} & \sigma_{11} \sigma_{22} + \sigma_{12}^2
\end{array} \right)
$$

with Inverse

$$
\I_n^{-1}(\bm \theta) = \frac{1}{n} \left(\begin{array}{ccccc} 
\sigma_{11} & \cdots & \cdots & \cdots & \cdots \\
\sigma_{12} & \sigma_{22} & \cdots & \cdots & \cdots \\
0 & 0 & 2 \sigma_{11}^2 & \cdots & \cdots \\
0 & 0 & \frac{2 |\bm \Sigma|^2 (\sigma_{11} \sigma_{22} \sigma_{12}^2 - \sigma_{12}^4)}{c}  & 2 \sigma_{22}^2 & \cdots \\
0 & 0 & 2\sigma_{11} \sigma_{12} & 2\sigma_{22} \sigma_{12} & \frac{|\bm \Sigma|^2 (\sigma_{11}^2 \sigma_{22}^2 - \sigma_{12}^4)}{c}
\end{array} \right)
$$

where

$$
c := - \sigma_{12}^6 + 3 \sigma_{11} \sigma_{22} \sigma_{12}^4 - 3 (\sigma_{11} \sigma_{22} \sigma_{12})^2 + (\sigma_{11} \sigma_{22})^3.
$$

Now we have all the formulas needed to implement the EM and SEM algorithm for the bivariate normal example.

## Replication of the bivariate normal example from @meng1991sem

In this section we apply the EM and SEM algorithm to the bivariate normal example of section 4.4 from @meng1991sem.

We have given the following data:

```{r data}
# Data
x <- c(8,6,11,22,14,17,18,24,19,23,26,40,4,4,5,6,8,10)
y <- c(59,58,56,53,50,45,43,42,39,38,30,27,NA,NA,NA,NA,NA,NA)
```

One iteration of the EM algorithm for the bivariate normal example is implemented as follows:

```{r EM step}
em_step <- function(data, params) {
  # Do one em step.
  # Use parameters from iteration t and compute iteration t+1.
  #
  # Args:
  #   data:   2-dimensional data matrix, where data are missing in
  #           second component and first component is complete
  #   params: list of parameters containing mu and cov
  #
  # Returns:
  #   A list containing 6 parameters, 2 for mean and 4 for covariance matrix
  
  # Preparation
  mu <- params$mu
  cov <- params$cov
  x <- data[,1]
  y <- data[,2]
  indicator <- is.na(y)
  n <- length(x)
  
  ## E step, update data
  # Regression estimation
  beta_1 <- cov[1,2]/cov[1,1]
  beta_0 <- mu[2] - beta_1*mu[1]
  sig_y_x <- cov[2,2] - beta_1^2*cov[1,1]
  
  # Imputation
  y[indicator] <- beta_0 + beta_1*x[indicator]
  
  # Estimate sufficient statistics
  sum_y <- sum(y)
  sum_xy <- sum(x*y)
  sum_yy <- sum(c(y[indicator]^2 + sig_y_x, y[!indicator]^2))
  
  ## M step, update parameters
  mu[2] <- mean(y)
  
  cov[1,2] <- cov[2,1] <-  (1/n)*(sum_xy - (sum_y*sum(x))/n)
  cov[2,2] <- (1/n)*(sum_yy - sum_y^2/n)
  
  return(params = list(mu = mu, cov = cov))
}
```

```{r em, echo = FALSE}
initialize <- function(data) {
  # Initalize parameters for first em step.
  #
  # Args:
  #   data: 2-dimensional data matrix, where data are missing in
  #         second component and first component is complete
  #
  # Returns:
  #   A list containing 6 parameters, 2 for mean and 4 for covariance matrix
  
  # Preparation
  x <- data[,1]
  y <- na.omit(data[,2])
  n <- length(x)
  
  # Initalize parameters
  mu <- c(mean(x), mean(y))
  s_xx <- (1/n)*(sum(x^2) - sum(x)^2/n)
  s_yy <- (1/n)*(sum(y^2) - sum(y)^2/n)
  cov <- matrix(c(s_xx, 0, 0, s_yy), nrow = 2, ncol = 2)
  
  return(list(mu = mu, cov = cov))
}

objective <- function(data, params) {
  # Calculate observed log likelihood using observed date and given parameters.
  #
  # Args:
  #   data:   2-dimensional data matrix, where data are missing in
  #           second component and first component is complete
  #   params: list of parameters containing mu and cov
  #
  # Returns:
  #   Log likelihood value
  
  # Preparation
  indicator <- is.na(data[,2])
  
  # Calculate 2D log likelihood where x and y is observed
  if(sum(!indicator) > 0) {
    const <- -log(2*pi)
    det <- -(1/2)*log(det(params$cov))
    maha <- -(1/2)*mahalanobis(data[!indicator,], params$mu, params$cov)
    log1 <- sum(const + det + maha)
  } else {
    log1 <- 0
  }

  # Calculate 1D log likelihood where just x is observed
  if(sum(indicator) > 0) {
    const <- -(1/2)*log(2*pi)
    det <- -(1/2)*log(params$cov[1,1])
    maha <- -(1/2)*(data[indicator,][,1]-params$mu[1])^2/params$cov[1,1]
    log2 <- sum(const + det + maha)
  } else {
    log2 <- 0
  }
  
  # Sum both results to obtain observed log likelihood and return
  return(log1+log2)
}

em_algorithm <- function(data, tolerance) {
  # Run EM algorithm.
  #
  # Args:
  #   data:      2-dimensional data matrix, where data are missing in
  #              second component and first component is complete
  #   tolerance: convergence criterion
  #
  # Returns:
  #   List containing vector of log likelihoods for every step
  #   and list of parameters from all steps, including mle estimate,
  #   orresponding to last em step
  
  # Preparation
  params <- initialize(data)
  vecLoglike <- NULL
  lParams <- list(params)
  
  # Repeat as long as convergence criterion is hit
  repeat {
    # EM step
    params <- em_step(data, params)
    lParams <- append(lParams, list(params))
    
    # Calculate objective (likelihood)
    vecLoglike <- append(vecLoglike, objective(data, params))
    
    # Check if convergence criterion is hit
    last <- length(vecLoglike)
    if(last >= 2 && abs(vecLoglike[last] - vecLoglike[last-1]) < tolerance) {
      break
    }
  }
  
  return(list(loglike = vecLoglike, params = lParams))
}

# Paper example 3
data <- matrix(c(x,y), ncol = 2)

# Run algorithm
result <- em_algorithm(data, tolerance = 10^-12)
```

We get the following results for the parameters

```{r EM param, results="asis", echo=FALSE}
print(xtable(matrix(c(tail(result$params, 1)[[1]]$mu, 
                      tail(result$params, 1)[[1]]$cov[1,1],
                      tail(result$params, 1)[[1]]$cov[2,2],
                      tail(result$params, 1)[[1]]$cov[1,2]),
                    ncol = 5, dimnames = list(
                      "values",
                      c("mu1", "mu2", "sigma11", "sigma22", "sigma12"))
                    ),
             caption = "EM parameter estimates"),
      comment = FALSE)
```

which match the results in Table 5 of @meng1991sem of the transformed parameterization:

```{r EM transParam, results="asis", echo=FALSE}
corr <- tail(result$params, 1)[[1]]$cov[1, 2] / 
  sqrt(tail(result$params, 1)[[1]]$cov[1, 1] * tail(result$params, 1)[[1]]$cov[2, 2])
Z <- 0.5 * log((1+corr)/(1-corr))

print(xtable(matrix(c(tail(result$params, 1)[[1]]$mu[2], 
                      log(tail(result$params, 1)[[1]]$cov[2,2]), 
                      Z),
                    ncol = 3, dimnames = list(
                      "values",
                      c("mu2", "log sigma22", "Z"))
                    ),
             caption = "EM estimates of transformed parameters"),
      comment = FALSE)
```

```{r likelihood convergence, fig.cap = "Convergence of observed log likelihood. \\label{fig:likelihood-convergence}", echo = FALSE}
# Plot convergence of log likelihood
plot(result$loglike, type = "l", xlab = "Number of EM iterations", ylab = "Log-Likelihood")
grid()
```

In Figure \ref{fig:likelihood-convergence}, we can see that the observed log-likelihood converges to some maximum in the parameters from the EM iterations. Figure \ref{fig:contour} shows the densities of the bivariate normal at different stages of the EM algorithm.

```{r contour plot, fig.height = 6, fig.cap = "Contour plots of bivariate normal at different stages of the EM algorithm. \\label{fig:contour}", echo = FALSE}
# Contour plot
par(mfrow=c(2,2))
for(i in c(1,2,3,length(result$params))) {
  plot(data, pch = 20, xlim = c(-10,45), ylim = c(15,85),
       xlab = "x", ylab = "y", main = paste("EM iteration", i-1))
  grid()
  for(j in 1:9) ellipse(mu=result$params[[i]]$mu, sigma=result$params[[i]]$cov,
                        alpha = (j*0.1), npoints = 250, col=rgb(0,0,0,alpha=0.4))
}
```


```{r sem}
calculateRatio <- function(data, params, i, j, tolerance) {
  # Calculate ratio as entry for DM matrix.
  #
  # Args:
  #   data:   2-dimensional data matrix, where data are missing in
  #           second component and first component is complete
  #   params: list of parameters containing mu and cov
  #   i,j:    index of DM matrix
  #   tolerance: convergence criterion
  #
  # Returns:
  #   The convergence rate in matrix DM of index i,j
  
  # Preparation
  indicator <- is.na(data[,2])
  vecR <- NULL
  
  # Get last parameter element (MLE)
  mle <- unlist(tail(params, 1)[[1]])
  
  # Compute rate until convergence
  t <- 1
  repeat {
    # Get current state of theta from em computation
    theta_t <- unlist(params[[t]])
    
    # Define theta_i as the final mle and replace i-th value with current state
    theta_t_i <- mle
    theta_t_i[[i]] <- theta_t[[i]]
    
    # Do EM step using theta_i: convert parameters to list, then do step
    paramList <- list(mu = theta_t_i[1:2],
                      cov = matrix(theta_t_i[3:6], nrow = 2, ncol = 2))
    theta_t_1_i <- unlist(em_step(data, paramList))
    
    # Calculate ratio
    vecR <- append(vecR, (theta_t_1_i[[j]] - mle[[j]])/(theta_t[[i]] - mle[[i]]))
    
    # Increase iteration
    t <- t+1
    
    # Check if convergence criterion is hit or we're running out of original estimations
    last <- length(vecR)
    if((last >= 2 && abs(vecR[last] - vecR[last-1]) < tolerance)) {
      break
    }
    
    # Check if there is still a parameter from EM to calculate next iteration
    if(t >= length(params)) {
      warning("SEM did not converge for one component.")
      break
    }
  }
  
  # Just return last rate after convergence
  return(vecR[length(vecR)])
}

calculateDM <- function(data, params, tolerance) {
  # Calculate DM matrix, calling calculateRatio for every entry in matrix
  #
  # Args:
  #   data:   2-dimensional data matrix, where data are missing in
  #           second component and first component is complete
  #   params: list of parameters containing mu and cov
  #   tolerance: convergence criterion
  #
  # Returns:
  #   Whole DM matrix.
  
  # Parameters to estimate in DM*
  # 1: mu1, 2: mu2, 3: s_xx, 4/5: s_xy, 6: s_yy
  estimates <- c(2,4,6)
  
  # Number of parameters to calculate variance for
  d <- length(estimates)
  
  # Define empty DM matrix
  DM <- matrix(nrow = d, ncol = d)
  
  # Calculate any r_ij and store in DM
  for(i in 1:d) {
    for(j in 1:d) {
      DM[i,j] <- calculateRatio(data, params, estimates[i], estimates[j], tolerance)
    }
  }
  
  return(DM)
}

sem_algorithm <- function(data, params, tolerance) {
  # Run whole SEM algorithm to obtain variances of EM estimators.
  #
  # Args:
  #   data:   2-dimensional data matrix, where data are missing in
  #           second component and first component is complete
  #   params: list of parameters containing mu and cov
  #   tolerance: convergence criterion
  #
  # Returns:
  #   Matrix V.
  
  # Preparation
  n <- length(data[,1])
  
  # Get DM* matrix
  DM <- calculateDM(data, params, tolerance)
  
  # Get covariance matrix of MLE estimate (last step from em algorithm)
  cov <- tail(params, 1)[[1]]$cov
  
  # Preparation of c
  c <- -cov[1,2]^6+3*cov[1,1]*cov[2,2]*cov[1,2]^4-3*(cov[1,1]*cov[2,2]*cov[1,2])^2+(cov[1,1]*cov[2,2])^3
  
  # Calculate G1 of I_oc^-1
  G1_11 <- cov[1,1]
  G1_12 <- 0
  G1_22 <- 2*cov[1,1]^2
  
  G1 <- (1/n)*matrix(c(G1_11, G1_12, G1_12, G1_22), nrow = 2, ncol = 2)
  
  # Calculate G2 of I_oc^-1
  G2_11 <- cov[1,2]
  G2_12 <- 0
  G2_13 <- 0
  G2_21 <- 0
  G2_22 <- 2*cov[1,1]*cov[2,2]
  G2_23 <- 2*det(cov)^2*(cov[1,1]*cov[2,2]*cov[1,2]^2 - cov[1,2]^4)/c
  
  G2 <- (1/n)*matrix(c(G2_11, G2_12, G2_13, G2_21, G2_22, G2_23),
                     nrow = 2, ncol = 3, byrow = TRUE)
  
  # Calculate G3 of I_oc^-1
  G3_11 <- cov[2,2]
  G3_12 <- 0
  G3_13 <- 0
  G3_22 <- det(cov)^2*((cov[1,1]*cov[2,2])^2 - cov[1,2]^4)/c
  G3_23 <- 2*cov[2,2]*cov[1,2]
  G3_33 <- 2*cov[2,2]^2
  
  G3 <- (1/n)*matrix(c(G3_11, G3_12, G3_13, G3_12, G3_22, G3_23,
                       G3_13, G3_23, G3_33), nrow = 3, ncol = 3)
  
  # Compute Delta V*
  DV <- (G3 - t(G2)%*%solve(G1)%*%G2)%*%DM%*%solve(diag(3)-DM)
  
  # Compute V and return
  return(G3 + DV)
}

# Run SEM algorithm and get variance
V <- sem_algorithm(data, result$params, 10^-4)
```

## Simulation study

```{r sim, echo = FALSE}
simulation <- function(data, N, nSim, tolEM, tolSEM) {
  # Simulation with nSim simulations of N sized data.
  # Calculate variances of EM estimators using SEM and MC.
  #
  # Args:
  #   data: 2-dimensional data matrix, where data are missing in
  #         second component and first component is complete
  #   N:    size of data
  #   nSim: number of simulations
  #   tolEM:  convergence criterion for EM
  #   tolSEM: convergence criterion for SEM
  #
  # Returns:
  #   A list containing:
  #   * log-likelihoods of every simulation EM and of all EM steps
  #   * mle estimators of the final EM step
  #   * standard deviations od EM estimators, based on SEM and MC
  
  # Preparation
  results <- vector("list", length = nSim)
  loglikes <- vector("list", length = nSim)
  mles <- matrix(3*nSim, nrow = nSim, ncol = 3)
  
  # Run EM algorithm and store loglikelihoods and mle's
  for(iSim in 1:nSim) {
    results[iSim] <- list(em_algorithm(data[((iSim-1)*N+1):(iSim*N),], tolEM))
    loglikes[iSim] <- list(results[iSim][[1]]$loglike)
    mles[iSim,] <- unlist(tail(results[[iSim]]$params, 1)[[1]])[c(2,4,6)]
  }
  
  ## Calculate MC SD values
  mcSd <- apply(mles, 2, function(col) {sd(col)})
  names(mcSd) <- c("sd(mu2)","sd(cov12)","sd(cov22)")
  
  ## Calculate SEM SD values
  # Preparation
  semVars <- vector("list", length = nSim)
  semVars <- matrix(3*nSim, nrow = nSim, ncol = 3)
  
  # Run SEM for every simulation
  for(iSim in 1:nSim) {
    sem <- sem_algorithm(data[((iSim-1)*N+1):(iSim*N),], results[[iSim]]$params, tolSEM)
    semVars[iSim,] <- suppressWarnings(sqrt(diag(sem)))
  }
  
  # If any diagonal value is negative, alert warning
  if(sum(is.nan(semVars)) > 0) {
    warning("Sem V matrix probably not positive semidefinite. Probably reached saddle point. Increase EM iterations.")
  }
  
  # Calculate mean of SD estimations
  semSd <- apply(semVars, 2, function(col) {mean(col)})
  names(semSd) <- c("sd(mu2)","sd(cov12)","sd(cov22)")
  
  return(list(loglikes = loglikes, mles = mles, sd = list(mc = mcSd, sem = semSd)))
}

plotSDMu2 <- function(simResults, nSim) {
  # Plot standard deviation, estimated by MC and SEM for parameter mu2
  #
  # Args:
  #   simResults: results from simulated data
  #   nSim: number of simulations
  #
  # Returns:
  #   -
  
  # Preparation
  meanMu2 <- mean(simResults$mles[,1])
  mcSdMu2 <- simResults$sd$mc[[1]]
  semSdMu2 <- simResults$sd$sem[[1]]
  
  # Plot MC SD results
  plot(simResults$mles[,1], rep(1,nSim), pch = 16, col = rgb(0,0,0,0.1), cex = 0.8,
       ylim = c(-0.5,1.5), ylab = "", xlab = "", yaxt = 'n',
       main = "Standard deviation for mu2")
  axis(2, at = c(1,0), labels = c("MC", "SEM"))
  arrows(meanMu2-mcSdMu2,1,meanMu2+mcSdMu2,1, col = "blue", code=3, length=0.1, angle = 90, lwd = 2)
  points(meanMu2, 1, pch = 16, col = "red", cex = 1.2)
  
  # Plot SEM SD results
  points(simResults$mles[,1], rep(0,nSim), pch = 16, col = rgb(0,0,0,0.1), cex = 0.8)
  arrows(meanMu2-semSdMu2,0,meanMu2+semSdMu2,0, col = "blue", code=3, length=0.1, angle = 90, lwd = 2)
  points(meanMu2, 0, pch = 16, col = "red", cex = 1.2)
}
```

```{r plot functions, echo = FALSE}
plotLikeli <- function(simResults, nSim) {
  # Plot likelihood convergence of EM for simulated daza
  #
  # Args:
  #   simResults: results from simulated data
  #   nSim: number of simulations
  #
  # Returns:
  #   -
  
  # Preparation
  logs <- ldply(simResults$loglikes, rbind)
  nMaxIterations <- dim(logs)[2]
  for(i in 1:nSim) {
    lastEntry <- sum(!is.na(logs[i,]))
    logs[i,][lastEntry:nMaxIterations] <- logs[i,][lastEntry]
  }
  
  # Plot all likelihood convergence lines
  plot(1, xlab = "Iteration", ylab = "Observed loglikelihood",
       xlim = c(1,dim(logs)[2]),
       ylim = c(min(logs, na.rm = TRUE), max(logs, na.rm = TRUE)))
  for(i in 1:length(simResults$loglikes)) {
    lines(1:length(logs[i,]), logs[i,], col = rgb(0,0,1,0.08))
  }
  
  # Plot mean likelihodd
  lines(apply(logs, 2, mean, na.rm = TRUE), col = "red")
}

# Simulate 2D data
nSim <- 1000
N <- 250
data <- data_mar <- data_mcar <- 
  mvrnorm(N*nSim, c(100,12), matrix(c(169,19.5,19.5,9), nrow = 2, ncol = 2))

# Generate MCAR data
sequence <- sample(seq(1,N), N/2)
for(iSim in 1:nSim) {
  data_mcar[((iSim-1)*N+1):(iSim*N),][,2][sequence] <- NA
}
```

To compare the results of the SEM variance estimation with some other estimation, a simulation study was implemented. The simulation is based on @enders2010missing chapter 4.8. Enders used two variables which are normally distributed. The first variable $X$ was chosen as intelligence with $\mu_X = 100$ and $\sigma_{XX} = 169$. The second variable $Y$, job perfornance, follows a ditribution with $\mu_Y = 12$ and $\sigma_{YY} = 9$. The covariance between them should be $\sigma_{XY} = 19.5$.

Like Enders, we simulated $1000$ datasets, each having a size of $N = 250$. We obtained the variances, using SEM algorithm and MC estimation. Using the SEM algorithm, we obtained a variance estimation for every dataset, taking the quare rooted diagonal to get standard deviations. Finally the mean over all simulations was taken. In MC simulation, the ML estimators are ca 

```{r mcar ana, cache = TRUE}
# Get variance estimation with simulated data using SEM and MC under MCAR
results_mcar <- simulation(data_mcar, N, nSim, tolEM = 10^-8, tolSEM = 10^-4)

# Print results
results_mcar$sd
```

```{r mcar plots, echo = FALSE}
# Plot results
plotSDMu2(results_mcar, nSim)
plotLikeli(results_mcar, nSim)
```

```{r mar, cache = TRUE}
# Generate MAR data
data_mar[,2][data_mar[,1] < median(data_mar[,1])] <- NA

# Get variance estimation with simulated data using SEM and MC under MAR
results_mar <- simulation(data_mar, N, nSim, tolEM = 10^-8, tolSEM = 10^-4)

# Print results
results_mar$sd
```

```{r mar, echo = FALSE}
# Plot results
plotSDMu2(results_mar, nSim)
plotLikeli(results_mar, nSim)
```

\newpage

<!--Footnotes-->
[^1]: One can show that $L(\theta^{(t+1)} \, | \, \bm \Yobs) - L(\theta^{(t)} \, | \, \bm \Yobs) \geq 0$ using Jensen's inequality. See for example the original EM paper by @dempster1977maximum for a detailed proof.
[^2]: See for example <https://en.wikipedia.org/wiki/Fisher_information#Multivariate_normal_distribution>.
[^3]: Note that for ML estimators holds $(\bm\theta - \bm{\hat\theta}) \overset{d}{\longrightarrow} N(0,V)$ for $n \to\infty$, see ch. 9.1 in @little2014statistical.


<!-- References -->
# References
